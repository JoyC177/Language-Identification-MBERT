{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eed7c14",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import unicodedata\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f865116",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The languages in order they are classified by the classifiers on the build models.\n",
    "langs = ['ace', 'afr', 'als', 'amh', 'ang', 'ara', 'arg', 'arz', 'asm', 'ast', 'ava', 'aym', 'azb', 'aze', 'bak', 'bar', 'bcl', 'be-tarask', 'bel', 'ben', 'bho', 'bjn', 'bod', 'bos', 'bpy', 'bre', 'bul', 'bxr', 'cat', 'cbk', 'cdo', 'ceb', 'ces', 'che', 'chr', 'chv', 'ckb', 'cor', 'cos', 'crh', 'csb', 'cym', 'dan', 'deu', 'diq', 'div', 'dsb', 'dty', 'egl', 'ell', 'eng', 'epo', 'est', 'eus', 'ext', 'fao', 'fas', 'fin', 'fra', 'frp', 'fry', 'fur', 'gag', 'gla', 'gle', 'glg', 'glk', 'glv', 'grn', 'guj', 'hak', 'hat', 'hau', 'hbs', 'heb', 'hif', 'hin', 'hrv', 'hsb', 'hun', 'hye', 'ibo', 'ido', 'ile', 'ilo', 'ina', 'ind', 'isl', 'ita', 'jam', 'jav', 'jbo', 'jpn', 'kaa', 'kab', 'kan', 'kat', 'kaz', 'kbd', 'khm', 'kin', 'kir', 'koi', 'kok', 'kom', 'kor', 'krc', 'ksh', 'kur', 'lad', 'lao', 'lat', 'lav', 'lez', 'lij', 'lim', 'lin', 'lit', 'lmo', 'lrc', 'ltg', 'ltz', 'lug', 'lzh', 'mai', 'mal', 'map-bms', 'mar', 'mdf', 'mhr', 'min', 'mkd', 'mlg', 'mlt', 'mon', 'mri', 'mrj', 'msa', 'mwl', 'mya', 'myv', 'mzn', 'nan', 'nap', 'nav', 'nci', 'nds', 'nds-nl', 'nep', 'new', 'nld', 'nno', 'nob', 'nrm', 'nso', 'oci', 'olo', 'ori', 'orm', 'oss', 'pag', 'pam', 'pan', 'pap', 'pcd', 'pdc', 'pfl', 'pnb', 'pol', 'por', 'pus', 'que', 'roa-tara', 'roh', 'ron', 'rue', 'rup', 'rus', 'sah', 'san', 'scn', 'sco', 'sgs', 'sin', 'slk', 'slv', 'sme', 'sna', 'snd', 'som', 'spa', 'sqi', 'srd', 'srn', 'srp', 'stq', 'sun', 'swa', 'swe', 'szl', 'tam', 'tat', 'tcy', 'tel', 'tet', 'tgk', 'tgl', 'tha', 'ton', 'tsn', 'tuk', 'tur', 'tyv', 'udm', 'uig', 'ukr', 'urd', 'uzb', 'vec', 'vep', 'vie', 'vls', 'vol', 'vro', 'war', 'wln', 'wol', 'wuu', 'xho', 'xmf', 'yid', 'yor', 'zea', 'zh-yue', 'zho']\n",
    "\n",
    "# Language to index conversion\n",
    "language_to_index = {lang: i for i, lang in enumerate(langs)}\n",
    "index_to_language = {i: lang for i, lang in enumerate(langs)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d163f6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05084a5d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/Language-Identification_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b3374fc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "language_map = {\n",
    "    \"ar\": \"ara\",\n",
    "    \"bg\": \"bul\",\n",
    "    \"de\": \"deu\",\n",
    "    \"el\": \"ell\",\n",
    "    \"en\": \"eng\",\n",
    "    \"es\": \"spa\",\n",
    "    \"fr\": \"fra\",\n",
    "    \"hi\": \"hin\",\n",
    "    \"it\": \"ita\",\n",
    "    \"ja\": \"jpn\",\n",
    "    \"nl\": \"nld\",\n",
    "    \"pl\": \"pol\",\n",
    "    \"pt\": \"por\",\n",
    "    \"ru\": \"rus\",\n",
    "    \"sw\": \"swa\",\n",
    "    \"th\": \"tha\",\n",
    "    \"tr\": \"tur\",\n",
    "    \"ur\": \"urd\",\n",
    "    \"vi\": \"vie\",\n",
    "    \"zh\": \"zho\",\n",
    "}\n",
    "\n",
    "langs_orderd = list(language_map.values())\n",
    "langs_orderd.sort()\n",
    "\n",
    "df = df.replace({'labels':language_map})\n",
    "df.rename(columns = {'labels':'lang', 'text':'sentence'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e2fa24a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     lang                                           sentence\n",
      "0     nld                    Een man zingt en speelt gitaar.\n",
      "1     nld  De technologisch geplaatste Nasdaq Composite I...\n",
      "2     spa  Es muy resistente la parte trasera rígida y lo...\n",
      "3     ita  \"In tanti modi diversi, l'abilità artistica de...\n",
      "4     ara  منحدر يواجه العديد من النقاشات المتجهه إزاء ال...\n",
      "...   ...                                                ...\n",
      "9995  zho                               史料很充分，对岸的很多观点与大陆迥异啊。\n",
      "9996  tur  Örneğin, teşhis Yunanca bir kelimeden alındı (...\n",
      "9997  vie  Nếu lite/light chỉ đơn giản là mô tả một đặc t...\n",
      "9998  bul  Например, една щатска столица, която посетихме...\n",
      "9999  pol                   Mam dla ciebie kilka propozycji:\n",
      "\n",
      "[10000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b39485a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_test_id = [language_to_index[lang] for lang in df.lang]\n",
    "test = list(df[\"sentence\"])\n",
    "# , torch.Tensor(y_test_id).to(device).long())\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "test_dl = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29a8b34",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluation and Model Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62e2deca",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ClassifierHead(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim, output_dim, hidden_dims=None, activation=torch.nn.ReLU,\n",
    "        dropout_prob=0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = []\n",
    "\n",
    "        # Save arguments\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.activation = activation\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        dims = [input_dim] + hidden_dims + [output_dim]\n",
    "\n",
    "        # Construct layers\n",
    "        layers = []\n",
    "        for i_dim, o_dim in zip(dims, dims[1:]):\n",
    "            layers.append(torch.nn.Linear(i_dim, o_dim))\n",
    "            layers.append(self.activation)\n",
    "\n",
    "            if self.dropout_prob:\n",
    "                layers.append(torch.nn.Dropout(self.dropout_prob))\n",
    "        \n",
    "        if self.dropout_prob:\n",
    "            layers = layers[:-2]  # remove last activation and dropout\n",
    "        else:\n",
    "            layers = layers[:-1]  # remove last activation\n",
    "            \n",
    "        self.classifier = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "110a7bb5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def metadata_collector(sentences, device, features=list(), normalize=False):\n",
    "    data = torch.zeros((len(sentences), len(features))).to(device)\n",
    "    \n",
    "    for i, paragraph in enumerate(sentences):\n",
    "        for char in paragraph:\n",
    "            \n",
    "            cat = unicodedata.category(char)\n",
    "            \n",
    "            for idx in range(len(features)):\n",
    "                if cat in features[idx]:\n",
    "                    data[i][idx] += 1            \n",
    "    \n",
    "    # normalize the data to percentage of the sentence exists of\n",
    "    if normalize:\n",
    "        return torch.div(data.T, torch.sum(data, 1)).T\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "648e4dee",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_bert_embeddings(\n",
    "    model, tokenizer, data_loader, batch_size: int = 4\n",
    ") -> torch.Tensor:\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = []\n",
    "        # Loop over the sentences in batches\n",
    "        for sentences_batch in tqdm(data_loader):\n",
    "            encoded_input = tokenizer(\n",
    "                list(sentences_batch),\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(device)\n",
    "            output = model(**encoded_input)\n",
    "\n",
    "            # Take for the 0 layer the average of the tokens minus the CLS one\n",
    "            stacked = torch.stack(output.hidden_states)\n",
    "            \n",
    "            stacked[0,:,0,:] = (torch.sum(stacked[0], 1) - stacked[0,:,0,:]) / (stacked.shape[2]-1)\n",
    "\n",
    "            # Take [CLS] token embedding of specific layers or the last one.\n",
    "            cls_embeddings = stacked[:, :, 0, :].cpu()\n",
    "\n",
    "            # Store the embeddings\n",
    "            embeddings.append(cls_embeddings)\n",
    "\n",
    "    return torch.concat(embeddings, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1120c8ad",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class EmbeddingsDataset(Dataset):\n",
    "    def __init__(self, bert_embeddings, targets, features_embeddings=None):\n",
    "        self.bert_embeddings = bert_embeddings\n",
    "        self.features_embeddings = features_embeddings\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embeddings = [self.bert_embeddings[idx]]\n",
    "        if self.features_embeddings is not None:\n",
    "            embeddings.append(self.features_embeddings[idx])\n",
    "        return torch.cat(embeddings), self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e98db147",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dl, loss_fn):\n",
    "    # Evaluate\n",
    "    data_loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm_notebook(dl):\n",
    "            # Get batch\n",
    "            x, y = batch\n",
    "\n",
    "            # Get predictions\n",
    "            y_hat = model.forward(x)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(y_hat, y)\n",
    "\n",
    "            # Update loss\n",
    "            data_loss += loss.item()\n",
    "\n",
    "            # Update predictions\n",
    "            y_true.extend(y.tolist())\n",
    "            y_pred.extend(y_hat.argmax(dim=1).tolist())\n",
    "\n",
    "    # Calculate accuracy\n",
    "    acc = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "\n",
    "    return data_loss, acc, y_pred, y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c623bb5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load in the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "77a3bc2b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following experiment has been selected: mbert-bbs32-unic\n"
     ]
    }
   ],
   "source": [
    "# Replace the following string with the last bit of the Wandb.ai runpath\n",
    "# you want to take the model from.\n",
    "api = wandb.Api()\n",
    "\n",
    "path = \"nils2/dl4nlp/runs/\"\n",
    "run_path = path+\"1g90wn05\"\n",
    "\n",
    "run = api.run(run_path)\n",
    "run_args = run.config\n",
    "\n",
    "print(\"The following experiment has been selected:\",run.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7fb7bc77",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 13331, 'data_dir': 'data/wili-2018-split', 'features': ['Feature.UNICODE_CATEGORY'], 'momentum': 0.9, 'optimizer': 'Optimizer.SGD', 'activation': 'Activation.ReLU', 'batch_size': 1024, 'hidden_dims': [], 'dropout_prob': 0, 'learning_rate': 0.2, 'embeddings_dir': 'embeddings', 'bert_batch_size': 32, 'bert_model_name': 'bert-base-multilingual-cased', 'experiment_name': 'mbert-bbs32-unic', 'embeddings_layer': 12, 'unicode_categories': ['Ll', 'Zs', 'Lu', 'Po', 'Pd', 'Lo', 'Mn', 'Ps', 'Pe', 'Mc']}\n"
     ]
    }
   ],
   "source": [
    "print(run_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "39328b1c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = run_args['bert_model_name']\n",
    "\n",
    "# Load the correct model.\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, output_hidden_states=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "90553694",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features were detected in this run.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 313/313 [00:18<00:00, 17.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create the correct dataloader for the new dataset with the run parameters. \n",
    "if 'normalize' in run_args:\n",
    "    if run_args['normalize']:\n",
    "        print(\"Normalization flag was used this run.\")\n",
    "        normalize=True\n",
    "\n",
    "if run_args['features']:\n",
    "    print(\"Features were detected in this run.\")\n",
    "    features = run_args['unicode_categories']\n",
    "    BERT_DIM = len(features)\n",
    "    features_embeddings=metadata_collector(test, device, features, normalize=True)\n",
    "else:\n",
    "    features_embeddings=None\n",
    "    BERT_DIM = 0\n",
    "\n",
    "# Get the embeddings from the same layer.\n",
    "bert_embeddings = generate_bert_embeddings(model, tokenizer, test_dl, batch_size)[run_args['embeddings_layer']]\n",
    "BERT_DIM += bert_embeddings.shape[-1]\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        EmbeddingsDataset(bert_embeddings.to(device), \n",
    "                          torch.Tensor(y_test_id).to(device).long(), \n",
    "                          features_embeddings), \n",
    "        batch_size=batch_size, shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "acae4d76",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in the parameters from the corresponding saved file in WandB.\n",
    "best_model = wandb.restore('best_model.pth', run_path=run_path, replace=True)\n",
    "\n",
    "# use the \"name\" attribute of the returned object if your framework expects a filename, e.g. as in Keras\n",
    "LMclassifier = ClassifierHead(BERT_DIM, len(langs)).to(device)\n",
    "LMclassifier.load_state_dict(torch.load(best_model.name, map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836076c8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e0bde287",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83700e0786d84b93bed68a5f43f60498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_loss, test_accuracy, y_pred, y_true = evaluate(LMclassifier, test_loader, torch.nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b32a57a3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ara     1.0000    0.8540    0.9213       500\n",
      "         bul     0.9979    0.9640    0.9807       500\n",
      "         deu     1.0000    0.2440    0.3923       500\n",
      "         ell     1.0000    0.9940    0.9970       500\n",
      "         eng     0.9861    0.1420    0.2483       500\n",
      "         fra     1.0000    0.1980    0.3306       500\n",
      "         hin     1.0000    0.8820    0.9373       500\n",
      "         ita     1.0000    0.5500    0.7097       500\n",
      "         jpn     1.0000    0.9960    0.9980       500\n",
      "         nld     1.0000    0.7340    0.8466       500\n",
      "         pol     1.0000    0.9120    0.9540       500\n",
      "         por     1.0000    0.8800    0.9362       500\n",
      "         rus     0.9929    0.8340    0.9065       500\n",
      "         spa     1.0000    0.1520    0.2639       500\n",
      "         swa     0.9956    0.8960    0.9432       500\n",
      "         tha     1.0000    0.9440    0.9712       500\n",
      "         tur     1.0000    0.6880    0.8152       500\n",
      "         urd     0.9957    0.9360    0.9649       500\n",
      "         vie     1.0000    0.9860    0.9930       500\n",
      "         zho     0.9881    0.1660    0.2842       500\n",
      "\n",
      "   micro avg     0.9986    0.6976    0.8214     10000\n",
      "   macro avg     0.9978    0.6976    0.7697     10000\n",
      "weighted avg     0.9978    0.6976    0.7697     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred, labels=np.unique(y_true), target_names=langs_orderd, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "401cf532",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6976\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Print the model's failure modes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41636a84",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Print the paragraphs that were classified wrongly of the test set\n",
    "for i, (pred, true) in enumerate(zip(y_pred, y_true)):\n",
    "    if pred != true:\n",
    "        pred_lang, true_lang = index_to_language[pred], index_to_language[true]\n",
    "        print(f\"Paragraph {i} was classified as '{pred_lang}' but is actually '{true_lang}'\")\n",
    "        print(test[i])\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}