{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# (XL)MBERT Model Finetuning for Language Identification\n",
    "This notebook contains code to unfreeze the layers of BERT family models, in conjunction with training a linear probe on top for language identification (as has been done in our accompanying paper.\n",
    "\n",
    "We have chosen to include this code in a notebook rather than implement it in the framework of this repo, as this would require a major rewriting of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from math import sqrt\n",
    "import unicodedata\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Code to load the wili-2018 dataset\n",
    "def load_ds(path: str):\n",
    "    with open(path, encoding=\"utf8\") as f:\n",
    "        for l in f:\n",
    "            yield l.rstrip(\"\\n\")\n",
    "\n",
    "\n",
    "# Load the data\n",
    "x_train = load_ds(\"../data/wili-2018/x_train.txt\")\n",
    "y_train = load_ds(\"../data/wili-2018/y_train.txt\")\n",
    "x_test = load_ds(\"../data/wili-2018/x_test.txt\")\n",
    "y_test = load_ds(\"../data/wili-2018/y_test.txt\")\n",
    "\n",
    "x_train = pd.DataFrame(x_train, columns=[\"sentence\"])\n",
    "y_train = pd.DataFrame(y_train, columns=[\"lang\"])\n",
    "x_test = pd.DataFrame(x_test, columns=[\"sentence\"])\n",
    "y_test = pd.DataFrame(y_test, columns=[\"lang\"])\n",
    "\n",
    "# Preperation work for classification\n",
    "langs = sorted(y_train.lang.unique())\n",
    "\n",
    "language_to_index = {lang: i for i, lang in enumerate(langs)}\n",
    "index_to_language = {i: lang for i, lang in enumerate(langs)}\n",
    "\n",
    "y_train_id = [language_to_index[lang] for lang in y_train.lang]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# [!] Select your model\n",
    "Please select the model you want to finetune here. In the paper we have used \"bert-base-multilingual-cased\" (MBERT) and \"xlm-roberta-base\" as our DL approaches."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_name = \"xlm-roberta-base\"\n",
    "# model_name = \"bert-base-multilingual-cased\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## [1] Turn this to empty list to run without features (meta-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BERT_DIM = 768\n",
    "\n",
    "UNICODE_CATEGORIES = [\"Zs\", \"Po\", \"Lu\", \"Ll\", \"Pd\", \"Ps\", \"Pe\", \"Lo\", \"Mn\", \"Pf\"]\n",
    "# UNICODE_CATEGORIES = []\n",
    "\n",
    "DIM = len(UNICODE_CATEGORIES) + BERT_DIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Add features to the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/train_pre.csv')\n",
    "\n",
    "# Remove the \"useless\" features, decided to be useless in \"Feature Selection.ipynb\"\n",
    "df = df.drop(columns=['Nd', 'Cc', 'No', 'Nl', 'Co', 'Cn'])\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Lu   Ll   Zs  Po  Pd  Ps  Pe   Lo  Mn  Pf\n",
      "0        9  283   45   6   2   0   0    0   0   0\n",
      "1       17  120   31  11   3   3   3    0   0   0\n",
      "2        0    0   63   8   0   0   0  237  67   0\n",
      "3       42  750  159  37   1  16  15    0   0   4\n",
      "4        3   15   13   1   0   1   1  179  41   0\n",
      "...     ..  ...  ...  ..  ..  ..  ..  ...  ..  ..\n",
      "117495  26  792  187  27   0   3   3    0   0   0\n",
      "117496  14  200   45   9   1   0   0    0   0   0\n",
      "117497   0    0    0  34   0   1   1  195   0   0\n",
      "117498  19  495  106  16   0   0   0    0   0   0\n",
      "117499   6  238   46   4   0   0   0    0   0   0\n",
      "\n",
      "[117500 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "data = df.drop('Language', axis=1).drop('index', axis=1)\n",
    "\n",
    "# Drop all but interesting features.\n",
    "data = data[data.columns.intersection(UNICODE_CATEGORIES)]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Lu        Ll        Zs        Po        Pd        Ps        Pe  \\\n",
      "0       0.026087  0.820290  0.130435  0.017391  0.005797  0.000000  0.000000   \n",
      "1       0.090426  0.638298  0.164894  0.058511  0.015957  0.015957  0.015957   \n",
      "2       0.000000  0.000000  0.168000  0.021333  0.000000  0.000000  0.000000   \n",
      "3       0.041016  0.732422  0.155273  0.036133  0.000977  0.015625  0.014648   \n",
      "4       0.011811  0.059055  0.051181  0.003937  0.000000  0.003937  0.003937   \n",
      "...          ...       ...       ...       ...       ...       ...       ...   \n",
      "117495  0.025048  0.763006  0.180154  0.026012  0.000000  0.002890  0.002890   \n",
      "117496  0.052045  0.743494  0.167286  0.033457  0.003717  0.000000  0.000000   \n",
      "117497  0.000000  0.000000  0.000000  0.147186  0.000000  0.004329  0.004329   \n",
      "117498  0.029874  0.778302  0.166667  0.025157  0.000000  0.000000  0.000000   \n",
      "117499  0.020408  0.809524  0.156463  0.013605  0.000000  0.000000  0.000000   \n",
      "\n",
      "              Lo        Mn        Pf  \n",
      "0       0.000000  0.000000  0.000000  \n",
      "1       0.000000  0.000000  0.000000  \n",
      "2       0.632000  0.178667  0.000000  \n",
      "3       0.000000  0.000000  0.003906  \n",
      "4       0.704724  0.161417  0.000000  \n",
      "...          ...       ...       ...  \n",
      "117495  0.000000  0.000000  0.000000  \n",
      "117496  0.000000  0.000000  0.000000  \n",
      "117497  0.844156  0.000000  0.000000  \n",
      "117498  0.000000  0.000000  0.000000  \n",
      "117499  0.000000  0.000000  0.000000  \n",
      "\n",
      "[117500 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "norm_data = data.div(data.sum(axis=1), axis=0)\n",
    "print(norm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data['features'] = data.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Lu   Ll   Zs  Po  Pd  Ps  Pe   Lo  Mn  Pf  \\\n",
      "0        9  283   45   6   2   0   0    0   0   0   \n",
      "1       17  120   31  11   3   3   3    0   0   0   \n",
      "2        0    0   63   8   0   0   0  237  67   0   \n",
      "3       42  750  159  37   1  16  15    0   0   4   \n",
      "4        3   15   13   1   0   1   1  179  41   0   \n",
      "...     ..  ...  ...  ..  ..  ..  ..  ...  ..  ..   \n",
      "117495  26  792  187  27   0   3   3    0   0   0   \n",
      "117496  14  200   45   9   1   0   0    0   0   0   \n",
      "117497   0    0    0  34   0   1   1  195   0   0   \n",
      "117498  19  495  106  16   0   0   0    0   0   0   \n",
      "117499   6  238   46   4   0   0   0    0   0   0   \n",
      "\n",
      "                                      features  \n",
      "0            [9, 283, 45, 6, 2, 0, 0, 0, 0, 0]  \n",
      "1          [17, 120, 31, 11, 3, 3, 3, 0, 0, 0]  \n",
      "2           [0, 0, 63, 8, 0, 0, 0, 237, 67, 0]  \n",
      "3       [42, 750, 159, 37, 1, 16, 15, 0, 0, 4]  \n",
      "4          [3, 15, 13, 1, 0, 1, 1, 179, 41, 0]  \n",
      "...                                        ...  \n",
      "117495    [26, 792, 187, 27, 0, 3, 3, 0, 0, 0]  \n",
      "117496      [14, 200, 45, 9, 1, 0, 0, 0, 0, 0]  \n",
      "117497       [0, 0, 0, 34, 0, 1, 1, 195, 0, 0]  \n",
      "117498    [19, 495, 106, 16, 0, 0, 0, 0, 0, 0]  \n",
      "117499       [6, 238, 46, 4, 0, 0, 0, 0, 0, 0]  \n",
      "\n",
      "[117500 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a train dev split\n",
    "x_train, x_dev, y_train, y_dev = train_test_split(x_train, y_train_id, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "train_set = list(zip(x_train[\"sentence\"], torch.Tensor(y_train).to(device).long()))\n",
    "dev_set = list(zip(x_dev[\"sentence\"], torch.Tensor(y_dev).to(device).long()))\n",
    "\n",
    "y_test_id = [language_to_index[lang] for lang in y_test.lang]\n",
    "test = list(zip(x_test[\"sentence\"], torch.Tensor(y_test_id).to(device).long()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Diagnostic Classifier (/ Linear Probe)\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "class LinearDiagnosticClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Save dims\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Construct layer\n",
    "        self.layer = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def metadata_collector(sentences, device, features=list()):\n",
    "    \"\"\"\n",
    "    This function collects the metadata per sentence.\n",
    "    \"\"\"\n",
    "    data = torch.zeros((len(sentences), len(features))).to(device)\n",
    "    \n",
    "    for i, paragraph in enumerate(sentences):\n",
    "        for char in paragraph:\n",
    "            \n",
    "            cat = unicodedata.category(char)\n",
    "            \n",
    "            for idx in range(len(features)):\n",
    "                if cat in features[idx]:\n",
    "                    data[i][idx] += 1            \n",
    "    \n",
    "    # normalize the data to percentage of the sentence exists of\n",
    "    return torch.div(data.T, torch.sum(data, 1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "class TransformerLMwithClassifier(torch.nn.Module):\n",
    "    def __init__(self, transformer, classifier):\n",
    "        super().__init__()\n",
    "        self.transformer = transformer\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformer(**tokenizer(list(x), padding=True, truncation=True, return_tensors=\"pt\").to(device))[0][:, 0, :]\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "class TransformerLMwithClassifier_feature(torch.nn.Module):\n",
    "    def __init__(self, transformer, classifier):\n",
    "        super().__init__()\n",
    "        self.transformer = transformer\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        xfeat = metadata_collector(list(x), device, UNICODE_CATEGORIES)\n",
    "        x = self.transformer(**tokenizer(list(x), padding=True, truncation=True, return_tensors=\"pt\").to(device))[0][:, 0, :]\n",
    "        x = self.classifier(torch.cat((x, xfeat),1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PyTorch Training code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, train_dl, optimizer, loss_fn):\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch in tqdm_notebook(train_dl):\n",
    "        # Get batch\n",
    "        x, y = batch\n",
    "        \n",
    "        # Get predictions\n",
    "        y_hat = model.forward(x)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(y_hat, y)\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "\n",
    "def evaluate(model, dl, loss_fn):\n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    data_loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm_notebook(dl):\n",
    "            # Get batch\n",
    "            x, y = batch\n",
    "\n",
    "            # Get predictions\n",
    "            y_hat = model.forward(x)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(y_hat, y)\n",
    "\n",
    "            # Update loss\n",
    "            data_loss += loss.item()\n",
    "\n",
    "            # Update predictions\n",
    "            y_true.extend(y.tolist())\n",
    "            y_pred.extend(y_hat.argmax(dim=1).tolist())\n",
    "\n",
    "    # Calculate accuracy\n",
    "    acc = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "\n",
    "    return data_loss, acc, y_pred, y_true\n",
    "\n",
    "def train_model(model, train_set, dev_set, epochs=10, batch_size=32, lr=0.001, weight_decay=0.0, unfreeze_layers=1):\n",
    "    # Create dataloaders\n",
    "    train_dl = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    dev_dl = torch.utils.data.DataLoader(dev_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Create optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Create loss function\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Create scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, verbose=True)\n",
    "\n",
    "    # Unfreeze the last layer of the model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.transformer.encoder.layer[-unfreeze_layers:].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        train(model, train_dl, optimizer, loss_fn)\n",
    "\n",
    "        # Evaluate\n",
    "        # train_loss, train_accuracy, _, _ = evaluate(model, train_dl, loss_fn)\n",
    "        dev_loss, dev_accuracy, _, _ = evaluate(model, dev_dl, loss_fn)\n",
    "\n",
    "        # Print results\n",
    "        print(f\"Epoch {epoch + 1}\")\n",
    "        #print(f\"Train loss: {train_loss / len(train_dl):.4f}, accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Dev loss: {dev_loss / len(dev_dl):.4f}, accuracy: {dev_accuracy:.4f}\")\n",
    "        print()\n",
    "\n",
    "        # Update scheduler\n",
    "        scheduler.step(dev_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Loading the (previously trained) classifier\n",
    "Once you've trained the classifier in the cell below and save it, you can reload it later to do inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "loading_classifier = False # Be careful! Will overwrite the trained probe in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if loading_classifier:\n",
    "    if UNICODE_CATEGORIES:\n",
    "        LMclassifier = torch.load(\"networks/LMclassifier-Mbert-uni-norm.pt\")\n",
    "    else:\n",
    "        LMclassifier = torch.load(\"networks/LMclassifier-Mbert.pt\")\n",
    "else:\n",
    "    classifier = LinearDiagnosticClassifier(DIM, len(langs)).to(device)\n",
    "    \n",
    "    # Create the new classification model\n",
    "    if UNICODE_CATEGORIES:\n",
    "        LMclassifier = TransformerLMwithClassifier_feature(model, classifier)\n",
    "    else:\n",
    "        LMclassifier = TransformerLMwithClassifier(model, classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b12aa1623164bbea0ed9d37ea316cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c1468703a64d44b1e66dc4f507fa81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Dev loss: 0.3178, accuracy: 0.9168\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f73f8bac8a430eae83fda17bb8199c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920083e383d64118a8ad413fa9d218ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "Dev loss: 0.2650, accuracy: 0.9320\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f0b0d3fab74b8381333242d91e9cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "102194be27554b0a97d30d4be729dbd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "Dev loss: 0.2481, accuracy: 0.9372\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e2298d29b674ec287f5293172bb3af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc4a67063e349b78b52f197d4ad7048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n",
      "Dev loss: 0.2366, accuracy: 0.9379\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f54c9b2afdfa473dac61d28e59a48197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e6514093f1d40acb661aacf231a3526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n",
      "Dev loss: 0.2296, accuracy: 0.9406\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae33227f7b7a41b6a07acde686181fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf39b8e949b41578b357f0150c4dddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6\n",
      "Dev loss: 0.2263, accuracy: 0.9415\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b37210a7f74cd0b125a08137a0592c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154b312de1cf4bf9abadede93b1c20f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7\n",
      "Dev loss: 0.2191, accuracy: 0.9451\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae5cbb5b4ebd4b9580d3e4f8ceaffd00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4e7386155d4bb5b51da88011723624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8\n",
      "Dev loss: 0.2231, accuracy: 0.9433\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ebda9ffb414e35843ee8153a9f01cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb0e34185ad4c6a86a73dc0594b07c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9\n",
      "Dev loss: 0.2244, accuracy: 0.9436\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37b85a23774d423eaec96cddb530980d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93531a1975ee4ddb89e7a82cc26d26d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10\n",
      "Dev loss: 0.2195, accuracy: 0.9443\n",
      "\n",
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2e1d2d523f441fa3e1ee2a8a3b3053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1178cbbfc214ee8b42e5ed0ce4c7733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11\n",
      "Dev loss: 0.1852, accuracy: 0.9550\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a904b78f304a55a99bcb9862fc368a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf316be6054f42ea8a047f170ddf57ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12\n",
      "Dev loss: 0.1835, accuracy: 0.9568\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model (this was trained for 12 epochs)\n",
    "# if UNICODE_CATEGORIES :\n",
    "#     train_feature_model(LMclassifier, train_set, dev_set, epochs=12, batch_size=32, lr=0.001, weight_decay=0.0, unfreeze_layers=1)\n",
    "# else:\n",
    "train_model(LMclassifier, train_set, dev_set, epochs=12, batch_size=32, lr=0.001, weight_decay=0.0, unfreeze_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4efc7b177d11429aa044f7e77b02acd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3672 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Classification report on test set\n",
    "test_dl = torch.utils.data.DataLoader(test, batch_size=32, shuffle=False)\n",
    "\n",
    "# if UNICODE_CATEGORIES :\n",
    "#     test_loss, test_accuracy, y_pred, y_true = evaluate_feature(LMclassifier, test_dl, torch.nn.CrossEntropyLoss())\n",
    "# else:\n",
    "test_loss, test_accuracy, y_pred, y_true = evaluate(LMclassifier, test_dl, torch.nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ace     0.9820    0.9820    0.9820       500\n",
      "         afr     0.9920    0.9880    0.9900       500\n",
      "         als     0.8697    0.9080    0.8885       500\n",
      "         amh     0.9940    0.9920    0.9930       500\n",
      "         ang     0.9695    0.9520    0.9606       500\n",
      "         ara     0.9368    0.9780    0.9569       500\n",
      "         arg     0.9741    0.9780    0.9760       500\n",
      "         arz     0.9734    0.9500    0.9615       500\n",
      "         asm     0.9759    0.9700    0.9729       500\n",
      "         ast     0.9439    0.9420    0.9429       500\n",
      "         ava     0.8124    0.8920    0.8503       500\n",
      "         aym     0.9520    0.9520    0.9520       500\n",
      "         azb     0.9900    0.9940    0.9920       500\n",
      "         aze     0.9800    0.9800    0.9800       500\n",
      "         bak     0.9583    0.9640    0.9611       500\n",
      "         bar     0.8669    0.9120    0.8889       500\n",
      "         bcl     0.9834    0.9480    0.9654       500\n",
      "   be-tarask     0.9336    0.9000    0.9165       500\n",
      "         bel     0.8994    0.9300    0.9145       500\n",
      "         ben     0.9458    0.9420    0.9439       500\n",
      "         bho     0.9460    0.9460    0.9460       500\n",
      "         bjn     0.9506    0.9240    0.9371       500\n",
      "         bod     1.0000    0.9940    0.9970       500\n",
      "         bos     0.5616    0.5560    0.5588       500\n",
      "         bpy     1.0000    0.9980    0.9990       500\n",
      "         bre     0.9960    0.9840    0.9899       500\n",
      "         bul     0.9857    0.9620    0.9737       500\n",
      "         bxr     0.9413    0.9300    0.9356       500\n",
      "         cat     0.9373    0.9560    0.9465       500\n",
      "         cbk     0.8498    0.8600    0.8549       500\n",
      "         cdo     0.9980    0.9880    0.9930       500\n",
      "         ceb     0.9901    1.0000    0.9950       500\n",
      "         ces     0.9919    0.9780    0.9849       500\n",
      "         che     1.0000    0.9940    0.9970       500\n",
      "         chr     0.9838    0.9700    0.9768       500\n",
      "         chv     0.9721    0.9740    0.9730       500\n",
      "         ckb     1.0000    1.0000    1.0000       500\n",
      "         cor     0.9859    0.9820    0.9840       500\n",
      "         cos     0.9620    0.9620    0.9620       500\n",
      "         crh     0.9682    0.9740    0.9711       500\n",
      "         csb     0.9763    0.9900    0.9831       500\n",
      "         cym     0.9900    0.9900    0.9900       500\n",
      "         dan     0.9722    0.9780    0.9751       500\n",
      "         deu     0.8760    0.9180    0.8965       500\n",
      "         diq     0.9295    0.9760    0.9522       500\n",
      "         div     0.9782    0.9880    0.9831       500\n",
      "         dsb     0.9395    0.9620    0.9506       500\n",
      "         dty     0.9465    0.8140    0.8753       500\n",
      "         egl     0.9656    0.9540    0.9598       500\n",
      "         ell     0.9960    0.9900    0.9930       500\n",
      "         eng     0.6374    0.9000    0.7463       500\n",
      "         epo     0.9858    0.9740    0.9799       500\n",
      "         est     0.9515    0.9800    0.9655       500\n",
      "         eus     0.9980    0.9980    0.9980       500\n",
      "         ext     0.9360    0.9360    0.9360       500\n",
      "         fao     0.9919    0.9760    0.9839       500\n",
      "         fas     0.8949    0.9880    0.9392       500\n",
      "         fin     0.9840    0.9860    0.9850       500\n",
      "         fra     0.8336    0.8820    0.8571       500\n",
      "         frp     0.9557    0.9500    0.9529       500\n",
      "         fry     0.9919    0.9820    0.9869       500\n",
      "         fur     0.9710    0.9360    0.9532       500\n",
      "         gag     0.9633    0.9440    0.9535       500\n",
      "         gla     0.9798    0.9720    0.9759       500\n",
      "         gle     0.9840    0.9820    0.9830       500\n",
      "         glg     0.9816    0.9600    0.9707       500\n",
      "         glk     0.9699    0.9020    0.9347       500\n",
      "         glv     0.9960    0.9920    0.9940       500\n",
      "         grn     0.9960    0.9840    0.9899       500\n",
      "         guj     0.9820    0.9840    0.9830       500\n",
      "         hak     0.9753    0.9480    0.9615       500\n",
      "         hat     0.9980    0.9820    0.9899       500\n",
      "         hau     0.9980    0.9840    0.9909       500\n",
      "         hbs     0.4938    0.5560    0.5230       500\n",
      "         heb     0.9940    0.9960    0.9950       500\n",
      "         hif     0.9375    0.9300    0.9337       500\n",
      "         hin     0.9548    0.9720    0.9633       500\n",
      "         hrv     0.6045    0.5380    0.5693       500\n",
      "         hsb     0.9527    0.9260    0.9391       500\n",
      "         hun     0.9940    0.9920    0.9930       500\n",
      "         hye     0.9939    0.9780    0.9859       500\n",
      "         ibo     0.9706    0.9260    0.9478       500\n",
      "         ido     0.9820    0.9820    0.9820       500\n",
      "         ile     0.9718    0.9640    0.9679       500\n",
      "         ilo     0.9568    0.9740    0.9653       500\n",
      "         ina     0.9681    0.9720    0.9701       500\n",
      "         ind     0.8250    0.8960    0.8591       500\n",
      "         isl     0.9744    0.9900    0.9821       500\n",
      "         ita     0.9241    0.9740    0.9484       500\n",
      "         jam     0.9860    0.9840    0.9850       500\n",
      "         jav     0.9511    0.9340    0.9425       500\n",
      "         jbo     1.0000    1.0000    1.0000       500\n",
      "         jpn     0.9745    0.9940    0.9842       500\n",
      "         kaa     0.9878    0.9720    0.9798       500\n",
      "         kab     0.9524    0.9600    0.9562       500\n",
      "         kan     0.9980    0.9780    0.9879       500\n",
      "         kat     0.9860    0.9840    0.9850       500\n",
      "         kaz     0.9900    0.9920    0.9910       500\n",
      "         kbd     1.0000    0.9940    0.9970       500\n",
      "         khm     0.9652    0.9440    0.9545       500\n",
      "         kin     0.9508    0.9660    0.9583       500\n",
      "         kir     0.9920    0.9880    0.9900       500\n",
      "         koi     0.9002    0.8840    0.8920       500\n",
      "         kok     0.9778    0.9680    0.9729       500\n",
      "         kom     0.8946    0.9000    0.8973       500\n",
      "         kor     1.0000    0.9920    0.9960       500\n",
      "         krc     0.9797    0.9660    0.9728       500\n",
      "         ksh     0.9629    0.9340    0.9482       500\n",
      "         kur     0.9938    0.9600    0.9766       500\n",
      "         lad     0.9469    0.9620    0.9544       500\n",
      "         lao     0.9491    0.9320    0.9405       500\n",
      "         lat     0.9331    0.9480    0.9405       500\n",
      "         lav     0.9859    0.9760    0.9809       500\n",
      "         lez     0.9677    0.9600    0.9639       500\n",
      "         lij     0.9678    0.9620    0.9649       500\n",
      "         lim     0.9599    0.9580    0.9590       500\n",
      "         lin     0.9422    0.9460    0.9441       500\n",
      "         lit     0.9726    0.9940    0.9832       500\n",
      "         lmo     0.9339    0.9320    0.9329       500\n",
      "         lrc     0.9913    0.9080    0.9478       500\n",
      "         ltg     0.9879    0.9820    0.9850       500\n",
      "         ltz     0.9630    0.9380    0.9504       500\n",
      "         lug     0.9876    0.9560    0.9715       500\n",
      "         lzh     0.9704    0.9840    0.9772       500\n",
      "         mai     0.9936    0.9360    0.9640       500\n",
      "         mal     0.9960    0.9880    0.9920       500\n",
      "     map-bms     0.8963    0.8300    0.8619       500\n",
      "         mar     0.9899    0.9780    0.9839       500\n",
      "         mdf     0.9649    0.9360    0.9503       500\n",
      "         mhr     0.9377    0.9640    0.9507       500\n",
      "         min     0.9901    1.0000    0.9950       500\n",
      "         mkd     0.9899    0.9820    0.9859       500\n",
      "         mlg     0.9980    1.0000    0.9990       500\n",
      "         mlt     0.9802    0.9900    0.9851       500\n",
      "         mon     0.9842    0.9960    0.9901       500\n",
      "         mri     0.9960    0.9940    0.9950       500\n",
      "         mrj     0.9939    0.9740    0.9838       500\n",
      "         msa     0.9323    0.9360    0.9341       500\n",
      "         mwl     0.9821    0.9860    0.9840       500\n",
      "         mya     1.0000    0.9960    0.9980       500\n",
      "         myv     0.8907    0.8960    0.8933       500\n",
      "         mzn     0.9567    0.9720    0.9643       500\n",
      "         nan     0.9940    0.9920    0.9930       500\n",
      "         nap     0.9715    0.9560    0.9637       500\n",
      "         nav     0.9980    1.0000    0.9990       500\n",
      "         nci     0.9605    0.9720    0.9662       500\n",
      "         nds     0.9776    0.9580    0.9677       500\n",
      "      nds-nl     0.9546    0.9260    0.9401       500\n",
      "         nep     0.8387    0.9460    0.8891       500\n",
      "         new     0.9940    0.9940    0.9940       500\n",
      "         nld     0.9535    0.9840    0.9685       500\n",
      "         nno     0.9684    0.9800    0.9742       500\n",
      "         nob     0.9816    0.9600    0.9707       500\n",
      "         nrm     0.9643    0.9720    0.9681       500\n",
      "         nso     0.9621    0.9640    0.9630       500\n",
      "         oci     0.9129    0.8800    0.8961       500\n",
      "         olo     0.9859    0.9820    0.9840       500\n",
      "         ori     0.9760    0.9740    0.9750       500\n",
      "         orm     0.9798    0.9720    0.9759       500\n",
      "         oss     0.9960    0.9960    0.9960       500\n",
      "         pag     0.9782    0.8960    0.9353       500\n",
      "         pam     0.9535    0.9440    0.9487       500\n",
      "         pan     1.0000    0.9860    0.9930       500\n",
      "         pap     0.9876    0.9580    0.9726       500\n",
      "         pcd     0.7988    0.7940    0.7964       500\n",
      "         pdc     0.8712    0.8520    0.8615       500\n",
      "         pfl     0.8569    0.8620    0.8594       500\n",
      "         pnb     0.9980    0.9820    0.9899       500\n",
      "         pol     0.9940    0.9920    0.9930       500\n",
      "         por     0.9159    0.9800    0.9469       500\n",
      "         pus     0.9958    0.9580    0.9766       500\n",
      "         que     0.9795    0.9560    0.9676       500\n",
      "    roa-tara     0.9677    0.9600    0.9639       500\n",
      "         roh     0.9657    0.9580    0.9618       500\n",
      "         ron     0.9783    0.9920    0.9851       500\n",
      "         rue     0.9816    0.9620    0.9717       500\n",
      "         rup     0.9641    0.9140    0.9384       500\n",
      "         rus     0.8269    0.9460    0.8825       500\n",
      "         sah     0.9959    0.9780    0.9869       500\n",
      "         san     0.9861    0.9940    0.9900       500\n",
      "         scn     0.9682    0.9740    0.9711       500\n",
      "         sco     0.9799    0.9760    0.9780       500\n",
      "         sgs     1.0000    0.9980    0.9990       500\n",
      "         sin     0.9917    0.9600    0.9756       500\n",
      "         slk     0.9783    0.9920    0.9851       500\n",
      "         slv     0.9741    0.9760    0.9750       500\n",
      "         sme     0.9860    0.9840    0.9850       500\n",
      "         sna     0.9280    0.9800    0.9533       500\n",
      "         snd     0.9980    0.9900    0.9940       500\n",
      "         som     0.9760    0.9780    0.9770       500\n",
      "         spa     0.8477    0.8460    0.8468       500\n",
      "         sqi     0.9920    0.9900    0.9910       500\n",
      "         srd     0.9676    0.9560    0.9618       500\n",
      "         srn     0.9899    0.9760    0.9829       500\n",
      "         srp     0.9429    0.9240    0.9333       500\n",
      "         stq     0.9938    0.9660    0.9797       500\n",
      "         sun     0.9877    0.9620    0.9747       500\n",
      "         swa     0.9323    0.9640    0.9479       500\n",
      "         swe     0.9862    1.0000    0.9930       500\n",
      "         szl     0.9899    0.9840    0.9870       500\n",
      "         tam     0.9900    0.9880    0.9890       500\n",
      "         tat     0.9623    0.9700    0.9661       500\n",
      "         tcy     0.9940    0.9980    0.9960       500\n",
      "         tel     0.9878    0.9720    0.9798       500\n",
      "         tet     0.9652    0.9440    0.9545       500\n",
      "         tgk     0.9919    0.9800    0.9859       500\n",
      "         tgl     0.9705    0.9880    0.9792       500\n",
      "         tha     0.9565    0.9680    0.9622       500\n",
      "         ton     0.9940    0.9940    0.9940       500\n",
      "         tsn     0.9662    0.9720    0.9691       500\n",
      "         tuk     0.9764    0.9920    0.9841       500\n",
      "         tur     0.9457    0.9760    0.9606       500\n",
      "         tyv     0.9799    0.9740    0.9769       500\n",
      "         udm     0.9898    0.9700    0.9798       500\n",
      "         uig     1.0000    0.9920    0.9960       500\n",
      "         ukr     0.9820    0.9800    0.9810       500\n",
      "         urd     0.9880    0.9860    0.9870       500\n",
      "         uzb     0.9900    0.9920    0.9910       500\n",
      "         vec     0.9420    0.9420    0.9420       500\n",
      "         vep     0.9744    0.9880    0.9811       500\n",
      "         vie     0.9960    0.9860    0.9910       500\n",
      "         vls     0.9315    0.9240    0.9277       500\n",
      "         vol     0.9960    0.9900    0.9930       500\n",
      "         vro     0.9797    0.9660    0.9728       500\n",
      "         war     0.9940    0.9960    0.9950       500\n",
      "         wln     0.9801    0.9840    0.9820       500\n",
      "         wol     0.9643    0.9720    0.9681       500\n",
      "         wuu     0.9481    0.9140    0.9308       500\n",
      "         xho     0.9733    0.9460    0.9594       500\n",
      "         xmf     0.9979    0.9660    0.9817       500\n",
      "         yid     0.9939    0.9780    0.9859       500\n",
      "         yor     0.9150    0.9260    0.9205       500\n",
      "         zea     0.9044    0.9460    0.9247       500\n",
      "      zh-yue     0.9335    0.9260    0.9297       500\n",
      "         zho     0.9297    0.9780    0.9532       500\n",
      "\n",
      "    accuracy                         0.9567    117500\n",
      "   macro avg     0.9579    0.9567    0.9570    117500\n",
      "weighted avg     0.9579    0.9567    0.9570    117500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=langs, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Saving the full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "saving_classifier = True # Be careful! Will overwrite the saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if saving_classifier:\n",
    "    # Save train embeddings to disk as a python pickle\n",
    "    # Create a directory for the embeddings if it does not exist yet\n",
    "    if not os.path.exists(\"networks\"):\n",
    "        os.mkdir(\"networks\")\n",
    "    torch.save(LMclassifier, \"networks/LMclassifier-XLMbert-uni.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Print the model's failure modes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Print the paragraphs that were classified wrongly of the test set\n",
    "for i, (pred, true) in enumerate(zip(y_pred, y_true)):\n",
    "    if pred != true:\n",
    "        pred_lang, true_lang = index_to_language[pred], index_to_language[true]\n",
    "        print(f\"Paragraph {i} was classified as '{pred_lang}' but is actually '{true_lang}'\")\n",
    "        print(x_test.iloc[i].sentence)\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}