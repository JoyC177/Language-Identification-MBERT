{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MBERT Linear Probe Training for Language Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as ex\n",
    "import torch\n",
    "import skorch\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skorch import NeuralNet\n",
    "from skorch.helper import predefined_split\n",
    "from skorch.dataset import Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm.auto import trange\n",
    "\n",
    "from meta_collector import metadata_collector\n",
    "\n",
    "\n",
    "def load_ds(path: str):\n",
    "    with open(path, encoding=\"utf8\") as f:\n",
    "        for l in f:\n",
    "            yield l.rstrip(\"\\n\")\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(13331)\n",
    "    \n",
    "x_train = load_ds(\"data/wili-2018/x_train.txt\")\n",
    "y_train = load_ds(\"data/wili-2018/y_train.txt\")\n",
    "x_test = load_ds(\"data/wili-2018/x_test.txt\")\n",
    "y_test = load_ds(\"data/wili-2018/y_test.txt\")\n",
    "\n",
    "x_train = pd.DataFrame(x_train, columns=[\"sentence\"])\n",
    "y_train = pd.DataFrame(y_train, columns=[\"lang\"])\n",
    "x_test = pd.DataFrame(x_test, columns=[\"sentence\"])\n",
    "y_test = pd.DataFrame(y_test, columns=[\"lang\"])\n",
    "\n",
    "# Create a train dev split\n",
    "x_train, x_dev, y_train, y_dev = train_test_split(x_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "train = pd.concat([x_train, y_train], axis=1)\n",
    "dev = pd.concat([x_dev, y_dev], axis=1)\n",
    "test = pd.concat([x_test, y_test], axis=1)\n",
    "langs = sorted(y_train.lang.unique())\n",
    "chars = set(c for s in train.sentence for c in s)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change here between feature version or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "778\n"
     ]
    }
   ],
   "source": [
    "# Switch here by uncommenting and commenting\n",
    "features = [\"Ll\", \"Zs\", \"Lu\", \"Po\", \"Pd\", \"Lo\", \"Mn\", \"Ps\", \"Pe\", \"Mc\"]\n",
    "# features = []\n",
    "\n",
    "BERT_DIM = 768\n",
    "h_dim = BERT_DIM + len(features)\n",
    "print(h_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_bert_embedding(model, tokenizer, sentences, batch_size=4, shrinkage_fact=1):\n",
    "    model.eval()\n",
    "    sentences = sentences[:len(sentences) // shrinkage_fact]\n",
    "    with torch.no_grad():\n",
    "        # Create the tensor to house the CLS embeddings\n",
    "        embeddings = torch.zeros((len(sentences), BERT_DIM)).to(device)\n",
    "\n",
    "        # Loop over the sentences in batches\n",
    "        for i in trange(0, len(sentences), batch_size):\n",
    "            encoded_input = tokenizer(sentences[i:i+batch_size], padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "            output = model(**encoded_input)\n",
    "            # Take [CLS] token embedding\n",
    "            last_hidden_states = output[0][:, 0, :]\n",
    "\n",
    "            # Store the embeddings\n",
    "            embeddings[i:i+len(last_hidden_states)] = last_hidden_states\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def get_features_embeddings(sentences, features, batch_size=4, shrinkage_fact=1):\n",
    "    sentences = sentences[:len(sentences) // shrinkage_fact]    \n",
    "    embeddings = torch.zeros((len(sentences), len(features)))\n",
    "    \n",
    "    for i in trange(0, len(sentences), batch_size):\n",
    "        meta = metadata_collector(sentences[i:i+batch_size], device, features)\n",
    "        embeddings[i:i+len(meta)] = meta\n",
    "\n",
    "    return embeddings.to(device)\n",
    "\n",
    "\n",
    "def extend_embeddings(bert_embeddings, features_embeddings):\n",
    "    return torch.cat((bert_embeddings, features_embeddings), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BERT embeddings.\n",
      "Loaded FEATURES embeddings.\n"
     ]
    }
   ],
   "source": [
    "loading_embeddings = True\n",
    "\n",
    "if loading_embeddings:\n",
    "    bert_train_embeddings = torch.from_numpy(np.load(\"embeddings/bert_train_embeddings.npy\")).to(device)\n",
    "    bert_dev_embeddings = torch.from_numpy(np.load(\"embeddings/bert_dev_embeddings.npy\")).to(device)\n",
    "    print(\"Loaded BERT embeddings.\")\n",
    "\n",
    "    if features:\n",
    "        features_train_embeddings = torch.from_numpy(np.load(\"embeddings/features_train_embeddings.npy\")).to(device)\n",
    "        features_dev_embeddings = torch.from_numpy(np.load(\"embeddings/features_dev_embeddings.npy\")).to(device)\n",
    "        print(\"Loaded FEATURES embeddings.\")\n",
    "else:\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "    model = BertModel.from_pretrained(\"bert-base-multilingual-cased\").to(device)\n",
    "\n",
    "    bert_train_embeddings = get_bert_embedding(model, tokenizer, train.sentence, features, batch_size=24)\n",
    "    bert_dev_embeddings = get_bert_embedding(model, tokenizer, dev.sentence, features, batch_size=24)\n",
    "    train_embeddings = bert_train_embeddings\n",
    "    dev_embeddings = bert_dev_embeddings\n",
    "    \n",
    "    if features:\n",
    "        features_train_embeddings = get_features_embeddings(train.sentence, features, batch_size=4)\n",
    "        features_dev_embeddings = get_features_embeddings(dev.sentence, features, batch_size=4)\n",
    "\n",
    "if features:\n",
    "    train_embeddings = extend_embeddings(bert_train_embeddings, features_train_embeddings)\n",
    "    dev_embeddings = extend_embeddings(bert_dev_embeddings, features_dev_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## SAVING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "saving_embeddings = False  # Be careful! Will overwrite the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if saving_embeddings:\n",
    "    # Save train embeddings to disk as a python pickle\n",
    "    # Create a directory for the embeddings if it does not exist yet\n",
    "    if not os.path.exists(\"embeddings\"):\n",
    "        os.mkdir(\"embeddings\")\n",
    "    \n",
    "    np.save(\"embeddings/bert_train_embeddings.npy\", bert_train_embeddings.cpu())\n",
    "    np.save(\"embeddings/bert_dev_embeddings.npy\", bert_dev_embeddings.cpu())\n",
    "\n",
    "    if features:\n",
    "        np.save(\"embeddings/features_train_embeddings.npy\", features_train_embeddings.cpu())\n",
    "        np.save(\"embeddings/features_dev_embeddings.npy\", features_dev_embeddings.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23500, torch.Size([23500, 778]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev.sentence), dev_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94000, torch.Size([94000, 778]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train.sentence), train_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# DIAGNOSTIC CLASSIFIER\n",
    "class LinearDiagnosticClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Save dims\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Construct layer\n",
    "        self.layer = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "language_to_index = {lang: i for i, lang in enumerate(langs)}\n",
    "index_to_language = {i: lang for i, lang in enumerate(langs)}\n",
    "\n",
    "y_dev_id = [language_to_index[lang] for lang in y_dev.lang]\n",
    "y_train_id = [language_to_index[lang] for lang in y_train.lang]\n",
    "valid_ds = Dataset(dev_embeddings, y_dev_id[:len(dev_embeddings)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.0506\u001b[0m        \u001b[32m1.1182\u001b[0m  2.3840\n",
      "      2        \u001b[36m0.8340\u001b[0m        \u001b[32m0.7943\u001b[0m  2.5227\n",
      "      3        \u001b[36m0.6255\u001b[0m        \u001b[32m0.6619\u001b[0m  2.7145\n",
      "      4        \u001b[36m0.5213\u001b[0m        \u001b[32m0.5926\u001b[0m  2.2552\n",
      "      5        \u001b[36m0.4548\u001b[0m        \u001b[32m0.5493\u001b[0m  2.2774\n",
      "      6        \u001b[36m0.4076\u001b[0m        \u001b[32m0.5187\u001b[0m  2.2728\n",
      "      7        \u001b[36m0.3718\u001b[0m        \u001b[32m0.4951\u001b[0m  2.4219\n",
      "      8        \u001b[36m0.3434\u001b[0m        \u001b[32m0.4763\u001b[0m  2.2622\n",
      "      9        \u001b[36m0.3201\u001b[0m        \u001b[32m0.4610\u001b[0m  2.2458\n",
      "     10        \u001b[36m0.3004\u001b[0m        \u001b[32m0.4485\u001b[0m  2.2264\n",
      "     11        \u001b[36m0.2836\u001b[0m        \u001b[32m0.4382\u001b[0m  2.2596\n",
      "     12        \u001b[36m0.2688\u001b[0m        \u001b[32m0.4296\u001b[0m  2.2469\n",
      "     13        \u001b[36m0.2557\u001b[0m        \u001b[32m0.4223\u001b[0m  2.3556\n",
      "     14        \u001b[36m0.2440\u001b[0m        \u001b[32m0.4161\u001b[0m  2.2917\n",
      "     15        \u001b[36m0.2335\u001b[0m        \u001b[32m0.4108\u001b[0m  2.2456\n",
      "     16        \u001b[36m0.2239\u001b[0m        \u001b[32m0.4061\u001b[0m  2.2398\n",
      "     17        \u001b[36m0.2151\u001b[0m        \u001b[32m0.4021\u001b[0m  2.4746\n",
      "     18        \u001b[36m0.2070\u001b[0m        \u001b[32m0.3985\u001b[0m  2.4426\n",
      "     19        \u001b[36m0.1995\u001b[0m        \u001b[32m0.3954\u001b[0m  2.7278\n",
      "     20        \u001b[36m0.1926\u001b[0m        \u001b[32m0.3926\u001b[0m  2.8533\n",
      "     21        \u001b[36m0.1861\u001b[0m        \u001b[32m0.3902\u001b[0m  3.0006\n",
      "     22        \u001b[36m0.1800\u001b[0m        \u001b[32m0.3880\u001b[0m  3.5792\n",
      "     23        \u001b[36m0.1744\u001b[0m        \u001b[32m0.3861\u001b[0m  4.3705\n",
      "     24        \u001b[36m0.1691\u001b[0m        \u001b[32m0.3843\u001b[0m  4.0833\n",
      "     25        \u001b[36m0.1641\u001b[0m        \u001b[32m0.3828\u001b[0m  3.6894\n",
      "     26        \u001b[36m0.1594\u001b[0m        \u001b[32m0.3815\u001b[0m  4.0290\n",
      "     27        \u001b[36m0.1549\u001b[0m        \u001b[32m0.3803\u001b[0m  4.2401\n",
      "     28        \u001b[36m0.1507\u001b[0m        \u001b[32m0.3792\u001b[0m  4.4082\n",
      "     29        \u001b[36m0.1468\u001b[0m        \u001b[32m0.3783\u001b[0m  4.8674\n",
      "     30        \u001b[36m0.1430\u001b[0m        \u001b[32m0.3774\u001b[0m  4.6971\n",
      "     31        \u001b[36m0.1394\u001b[0m        \u001b[32m0.3766\u001b[0m  3.9682\n",
      "     32        \u001b[36m0.1360\u001b[0m        \u001b[32m0.3759\u001b[0m  4.3400\n",
      "     33        \u001b[36m0.1327\u001b[0m        \u001b[32m0.3753\u001b[0m  5.7146\n",
      "     34        \u001b[36m0.1296\u001b[0m        \u001b[32m0.3747\u001b[0m  6.6487\n",
      "     35        \u001b[36m0.1267\u001b[0m        \u001b[32m0.3742\u001b[0m  5.1984\n",
      "     36        \u001b[36m0.1238\u001b[0m        \u001b[32m0.3737\u001b[0m  6.3204\n",
      "     37        \u001b[36m0.1211\u001b[0m        \u001b[32m0.3733\u001b[0m  6.3518\n",
      "     38        \u001b[36m0.1186\u001b[0m        \u001b[32m0.3729\u001b[0m  8.2739\n",
      "     39        \u001b[36m0.1161\u001b[0m        \u001b[32m0.3725\u001b[0m  5.7647\n",
      "     40        \u001b[36m0.1137\u001b[0m        \u001b[32m0.3722\u001b[0m  4.9098\n",
      "     41        \u001b[36m0.1114\u001b[0m        \u001b[32m0.3719\u001b[0m  6.5825\n",
      "     42        \u001b[36m0.1092\u001b[0m        \u001b[32m0.3717\u001b[0m  5.6352\n",
      "     43        \u001b[36m0.1071\u001b[0m        \u001b[32m0.3715\u001b[0m  6.4796\n",
      "     44        \u001b[36m0.1051\u001b[0m        \u001b[32m0.3713\u001b[0m  6.2520\n",
      "     45        \u001b[36m0.1032\u001b[0m        \u001b[32m0.3711\u001b[0m  7.0014\n",
      "     46        \u001b[36m0.1013\u001b[0m        \u001b[32m0.3709\u001b[0m  4.4800\n",
      "     47        \u001b[36m0.0995\u001b[0m        \u001b[32m0.3708\u001b[0m  4.8841\n",
      "     48        \u001b[36m0.0977\u001b[0m        \u001b[32m0.3706\u001b[0m  6.0276\n",
      "     49        \u001b[36m0.0960\u001b[0m        \u001b[32m0.3705\u001b[0m  6.5294\n",
      "     50        \u001b[36m0.0944\u001b[0m        \u001b[32m0.3704\u001b[0m  6.0778\n",
      "     51        \u001b[36m0.0928\u001b[0m        \u001b[32m0.3703\u001b[0m  5.9408\n",
      "     52        \u001b[36m0.0913\u001b[0m        \u001b[32m0.3703\u001b[0m  6.5402\n",
      "     53        \u001b[36m0.0898\u001b[0m        \u001b[32m0.3702\u001b[0m  4.5178\n",
      "     54        \u001b[36m0.0884\u001b[0m        \u001b[32m0.3701\u001b[0m  4.6711\n",
      "     55        \u001b[36m0.0870\u001b[0m        \u001b[32m0.3701\u001b[0m  5.7571\n",
      "     56        \u001b[36m0.0857\u001b[0m        \u001b[32m0.3701\u001b[0m  4.2345\n",
      "     57        \u001b[36m0.0844\u001b[0m        \u001b[32m0.3700\u001b[0m  4.3329\n",
      "     58        \u001b[36m0.0831\u001b[0m        \u001b[32m0.3700\u001b[0m  4.2643\n",
      "     59        \u001b[36m0.0819\u001b[0m        \u001b[32m0.3700\u001b[0m  4.6106\n",
      "     60        \u001b[36m0.0807\u001b[0m        \u001b[32m0.3699\u001b[0m  5.2445\n",
      "     61        \u001b[36m0.0795\u001b[0m        \u001b[32m0.3699\u001b[0m  4.5192\n",
      "     62        \u001b[36m0.0784\u001b[0m        \u001b[32m0.3699\u001b[0m  4.3910\n",
      "     63        \u001b[36m0.0773\u001b[0m        \u001b[32m0.3699\u001b[0m  4.1062\n",
      "     64        \u001b[36m0.0763\u001b[0m        0.3699  4.1406\n",
      "     65        \u001b[36m0.0752\u001b[0m        0.3699  4.5749\n",
      "     66        \u001b[36m0.0742\u001b[0m        0.3699  4.3152\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.net.NeuralNet'>[initialized](\n",
       "  module_=LinearDiagnosticClassifier(\n",
       "    (layer): Linear(in_features=778, out_features=235, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = NeuralNet(\n",
    "    module=LinearDiagnosticClassifier,\n",
    "    module__input_dim=train_embeddings.shape[-1],\n",
    "    module__output_dim=len(langs),\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    train_split=predefined_split(valid_ds),\n",
    "    max_epochs=1000,\n",
    "    device=device,\n",
    "    verbose=1,\n",
    "    optimizer = torch.optim.SGD,\n",
    "    optimizer__momentum=0.9,\n",
    "    optimizer__lr=0.2,\n",
    "#     optimizer = torch.optim.Adam,\n",
    "    callbacks=[\n",
    "        skorch.callbacks.EarlyStopping()\n",
    "    ]\n",
    ")\n",
    "\n",
    "net.fit(train_embeddings, y_train_id[:len(train_embeddings)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Save Linear Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "saving_classifier = False # Be careful! Will overwrite the saved probe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if saving_classifier:\n",
    "    # Save train embeddings to disk as a python pickle\n",
    "    # Create a directory for the embeddings if it does not exist yet\n",
    "    if not os.path.exists(\"networks\"):\n",
    "        os.mkdir(\"networks\")\n",
    "    \n",
    "    if features:\n",
    "        torch.save(net, \"networks/linear_probe_f.pt\")\n",
    "    else:\n",
    "        torch.save(net, \"networks/linear_probe.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Linear Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "loading_classifier = False # Be careful! Will overwrite the trained probe in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if loading_classifier:\n",
    "    if features:\n",
    "        net = torch.load(\"networks/linear_probe_f.pt\")\n",
    "    else:\n",
    "        net = torch.load(\"networks/linear_probe.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ace       0.96      0.97      0.97       100\n",
      "         afr       0.98      0.99      0.99       100\n",
      "         als       0.68      0.86      0.76       100\n",
      "         amh       0.98      0.94      0.96       100\n",
      "         ang       0.92      0.94      0.93       100\n",
      "         ara       0.89      0.97      0.93       100\n",
      "         arg       0.99      0.99      0.99       100\n",
      "         arz       0.97      0.88      0.92       100\n",
      "         asm       0.93      0.98      0.96       100\n",
      "         ast       0.92      0.98      0.95       100\n",
      "         ava       0.87      0.79      0.83       100\n",
      "         aym       0.92      0.89      0.90       100\n",
      "         azb       1.00      1.00      1.00       100\n",
      "         aze       0.99      0.98      0.98       100\n",
      "         bak       0.97      0.98      0.98       100\n",
      "         bar       0.85      0.85      0.85       100\n",
      "         bcl       0.94      0.93      0.93       100\n",
      "   be-tarask       0.69      0.79      0.74       100\n",
      "         bel       0.72      0.63      0.67       100\n",
      "         ben       0.98      0.95      0.96       100\n",
      "         bho       0.95      0.87      0.91       100\n",
      "         bjn       0.85      0.94      0.89       100\n",
      "         bod       1.00      0.99      0.99       100\n",
      "         bos       0.65      0.42      0.51       100\n",
      "         bpy       1.00      1.00      1.00       100\n",
      "         bre       0.99      0.96      0.97       100\n",
      "         bul       0.97      0.95      0.96       100\n",
      "         bxr       0.94      0.84      0.89       100\n",
      "         cat       0.97      0.97      0.97       100\n",
      "         cbk       0.74      0.82      0.78       100\n",
      "         cdo       0.95      0.95      0.95       100\n",
      "         ceb       1.00      1.00      1.00       100\n",
      "         ces       0.97      0.96      0.96       100\n",
      "         che       1.00      0.99      0.99       100\n",
      "         chr       0.98      0.98      0.98       100\n",
      "         chv       0.99      0.96      0.97       100\n",
      "         ckb       1.00      0.98      0.99       100\n",
      "         cor       0.99      0.98      0.98       100\n",
      "         cos       0.92      0.82      0.87       100\n",
      "         crh       0.93      0.94      0.94       100\n",
      "         csb       1.00      0.99      0.99       100\n",
      "         cym       0.98      0.99      0.99       100\n",
      "         dan       0.97      0.94      0.95       100\n",
      "         deu       0.92      0.92      0.92       100\n",
      "         diq       0.94      0.87      0.90       100\n",
      "         div       0.98      0.98      0.98       100\n",
      "         dsb       0.81      0.82      0.82       100\n",
      "         dty       0.62      0.73      0.67       100\n",
      "         egl       0.92      0.82      0.87       100\n",
      "         ell       0.98      1.00      0.99       100\n",
      "         eng       0.66      0.81      0.73       100\n",
      "         epo       0.91      0.96      0.94       100\n",
      "         est       0.92      0.98      0.95       100\n",
      "         eus       1.00      0.98      0.99       100\n",
      "         ext       0.88      0.84      0.86       100\n",
      "         fao       0.95      0.89      0.92       100\n",
      "         fas       0.90      0.95      0.93       100\n",
      "         fin       0.99      0.98      0.98       100\n",
      "         fra       0.81      0.83      0.82       100\n",
      "         frp       0.93      0.76      0.84       100\n",
      "         fry       0.97      0.96      0.96       100\n",
      "         fur       0.93      0.86      0.90       100\n",
      "         gag       0.88      0.92      0.90       100\n",
      "         gla       1.00      0.91      0.95       100\n",
      "         gle       0.93      0.99      0.96       100\n",
      "         glg       0.96      0.98      0.97       100\n",
      "         glk       0.84      0.91      0.87       100\n",
      "         glv       0.96      0.98      0.97       100\n",
      "         grn       1.00      1.00      1.00       100\n",
      "         guj       1.00      0.93      0.96       100\n",
      "         hak       0.98      0.93      0.95       100\n",
      "         hat       1.00      1.00      1.00       100\n",
      "         hau       0.98      0.97      0.97       100\n",
      "         hbs       0.48      0.63      0.55       100\n",
      "         heb       0.98      1.00      0.99       100\n",
      "         hif       0.90      0.90      0.90       100\n",
      "         hin       0.88      0.93      0.90       100\n",
      "         hrv       0.55      0.63      0.59       100\n",
      "         hsb       0.87      0.88      0.88       100\n",
      "         hun       1.00      0.99      0.99       100\n",
      "         hye       0.98      0.98      0.98       100\n",
      "         ibo       0.94      0.88      0.91       100\n",
      "         ido       0.98      0.95      0.96       100\n",
      "         ile       0.93      0.91      0.92       100\n",
      "         ilo       0.95      0.94      0.94       100\n",
      "         ina       0.80      0.90      0.85       100\n",
      "         ind       0.87      0.68      0.76       100\n",
      "         isl       0.90      0.95      0.93       100\n",
      "         ita       0.91      0.95      0.93       100\n",
      "         jam       0.98      0.94      0.96       100\n",
      "         jav       0.95      0.78      0.86       100\n",
      "         jbo       1.00      0.99      0.99       100\n",
      "         jpn       0.99      0.97      0.98       100\n",
      "         kaa       0.97      0.98      0.98       100\n",
      "         kab       0.88      0.92      0.90       100\n",
      "         kan       1.00      0.98      0.99       100\n",
      "         kat       0.93      0.93      0.93       100\n",
      "         kaz       1.00      1.00      1.00       100\n",
      "         kbd       1.00      0.99      0.99       100\n",
      "         khm       0.85      0.93      0.89       100\n",
      "         kin       0.80      0.92      0.86       100\n",
      "         kir       0.96      0.99      0.98       100\n",
      "         koi       0.79      0.69      0.74       100\n",
      "         kok       0.94      0.90      0.92       100\n",
      "         kom       0.69      0.75      0.72       100\n",
      "         kor       1.00      0.97      0.98       100\n",
      "         krc       0.95      0.93      0.94       100\n",
      "         ksh       0.87      0.88      0.88       100\n",
      "         kur       0.89      0.95      0.92       100\n",
      "         lad       0.95      0.87      0.91       100\n",
      "         lao       0.82      0.84      0.83       100\n",
      "         lat       0.97      0.94      0.95       100\n",
      "         lav       0.94      0.99      0.97       100\n",
      "         lez       0.87      0.97      0.92       100\n",
      "         lij       0.74      0.94      0.83       100\n",
      "         lim       0.79      0.84      0.82       100\n",
      "         lin       0.86      0.91      0.88       100\n",
      "         lit       1.00      0.98      0.99       100\n",
      "         lmo       0.97      0.95      0.96       100\n",
      "         lrc       0.87      0.86      0.86       100\n",
      "         ltg       0.98      0.96      0.97       100\n",
      "         ltz       1.00      0.97      0.98       100\n",
      "         lug       0.99      0.87      0.93       100\n",
      "         lzh       0.98      1.00      0.99       100\n",
      "         mai       0.83      0.97      0.89       100\n",
      "         mal       1.00      0.99      0.99       100\n",
      "     map-bms       0.71      0.88      0.79       100\n",
      "         mar       0.96      0.95      0.95       100\n",
      "         mdf       0.88      0.92      0.90       100\n",
      "         mhr       0.88      0.92      0.90       100\n",
      "         min       1.00      0.98      0.99       100\n",
      "         mkd       1.00      0.98      0.99       100\n",
      "         mlg       0.99      0.99      0.99       100\n",
      "         mlt       0.96      0.96      0.96       100\n",
      "         mon       0.95      0.96      0.96       100\n",
      "         mri       1.00      0.98      0.99       100\n",
      "         mrj       0.87      0.92      0.89       100\n",
      "         msa       0.90      0.93      0.92       100\n",
      "         mwl       0.90      0.93      0.92       100\n",
      "         mya       1.00      0.99      0.99       100\n",
      "         myv       0.76      0.79      0.77       100\n",
      "         mzn       0.94      0.85      0.89       100\n",
      "         nan       0.98      0.98      0.98       100\n",
      "         nap       0.84      0.80      0.82       100\n",
      "         nav       0.99      1.00      1.00       100\n",
      "         nci       0.93      0.96      0.95       100\n",
      "         nds       0.97      0.95      0.96       100\n",
      "      nds-nl       0.74      0.50      0.60       100\n",
      "         nep       0.77      0.57      0.66       100\n",
      "         new       0.97      0.97      0.97       100\n",
      "         nld       0.92      0.92      0.92       100\n",
      "         nno       0.96      0.94      0.95       100\n",
      "         nob       0.93      0.91      0.92       100\n",
      "         nrm       0.84      0.93      0.88       100\n",
      "         nso       0.99      0.91      0.95       100\n",
      "         oci       0.81      0.92      0.86       100\n",
      "         olo       0.96      0.95      0.95       100\n",
      "         ori       0.96      0.97      0.97       100\n",
      "         orm       0.90      0.90      0.90       100\n",
      "         oss       0.99      1.00      1.00       100\n",
      "         pag       0.95      0.88      0.91       100\n",
      "         pam       0.95      0.92      0.93       100\n",
      "         pan       1.00      0.99      0.99       100\n",
      "         pap       0.94      0.94      0.94       100\n",
      "         pcd       0.75      0.70      0.73       100\n",
      "         pdc       0.87      0.80      0.83       100\n",
      "         pfl       0.79      0.66      0.72       100\n",
      "         pnb       1.00      1.00      1.00       100\n",
      "         pol       0.97      0.98      0.98       100\n",
      "         por       1.00      0.98      0.99       100\n",
      "         pus       0.93      0.89      0.91       100\n",
      "         que       0.95      0.93      0.94       100\n",
      "    roa-tara       0.89      0.86      0.87       100\n",
      "         roh       0.80      0.95      0.87       100\n",
      "         ron       0.99      0.98      0.98       100\n",
      "         rue       0.90      0.92      0.91       100\n",
      "         rup       0.87      0.97      0.92       100\n",
      "         rus       0.81      0.94      0.87       100\n",
      "         sah       0.94      0.93      0.93       100\n",
      "         san       0.99      0.97      0.98       100\n",
      "         scn       0.89      0.96      0.92       100\n",
      "         sco       0.98      0.97      0.97       100\n",
      "         sgs       0.98      1.00      0.99       100\n",
      "         sin       0.93      0.87      0.90       100\n",
      "         slk       0.98      0.97      0.97       100\n",
      "         slv       0.98      0.98      0.98       100\n",
      "         sme       0.95      0.97      0.96       100\n",
      "         sna       0.88      0.91      0.90       100\n",
      "         snd       0.93      0.96      0.95       100\n",
      "         som       0.90      0.92      0.91       100\n",
      "         spa       0.82      0.75      0.79       100\n",
      "         sqi       1.00      0.98      0.99       100\n",
      "         srd       0.93      0.87      0.90       100\n",
      "         srn       0.96      0.97      0.97       100\n",
      "         srp       0.95      0.92      0.93       100\n",
      "         stq       0.92      0.92      0.92       100\n",
      "         sun       0.97      0.96      0.96       100\n",
      "         swa       0.93      0.92      0.92       100\n",
      "         swe       1.00      1.00      1.00       100\n",
      "         szl       0.94      0.92      0.93       100\n",
      "         tam       1.00      0.99      0.99       100\n",
      "         tat       1.00      1.00      1.00       100\n",
      "         tcy       0.97      1.00      0.99       100\n",
      "         tel       1.00      0.97      0.98       100\n",
      "         tet       0.89      0.91      0.90       100\n",
      "         tgk       1.00      0.98      0.99       100\n",
      "         tgl       0.96      0.94      0.95       100\n",
      "         tha       0.99      0.96      0.97       100\n",
      "         ton       0.98      0.98      0.98       100\n",
      "         tsn       0.91      0.98      0.94       100\n",
      "         tuk       1.00      0.97      0.98       100\n",
      "         tur       0.97      0.94      0.95       100\n",
      "         tyv       0.94      0.83      0.88       100\n",
      "         udm       0.86      0.87      0.87       100\n",
      "         uig       1.00      0.97      0.98       100\n",
      "         ukr       0.95      0.99      0.97       100\n",
      "         urd       1.00      0.97      0.98       100\n",
      "         uzb       0.99      0.99      0.99       100\n",
      "         vec       0.92      0.92      0.92       100\n",
      "         vep       0.95      0.93      0.94       100\n",
      "         vie       0.97      0.97      0.97       100\n",
      "         vls       0.81      0.79      0.80       100\n",
      "         vol       1.00      0.99      0.99       100\n",
      "         vro       0.98      0.93      0.95       100\n",
      "         war       1.00      1.00      1.00       100\n",
      "         wln       0.94      0.94      0.94       100\n",
      "         wol       0.94      0.91      0.92       100\n",
      "         wuu       0.94      0.80      0.86       100\n",
      "         xho       0.90      0.91      0.91       100\n",
      "         xmf       0.97      0.94      0.95       100\n",
      "         yid       0.99      0.95      0.97       100\n",
      "         yor       0.96      0.95      0.95       100\n",
      "         zea       0.67      0.85      0.75       100\n",
      "      zh-yue       0.88      0.85      0.86       100\n",
      "         zho       0.85      0.95      0.90       100\n",
      "\n",
      "    accuracy                           0.92     23500\n",
      "   macro avg       0.92      0.92      0.92     23500\n",
      "weighted avg       0.92      0.92      0.92     23500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_y_pred_id = np.argmax(net.predict(dev_embeddings), axis=1)\n",
    "dev_y_pred = [index_to_language[id] for id in dev_y_pred_id]\n",
    "print(classification_report(y_dev[:len(dev_y_pred)], dev_y_pred, target_names=langs, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('uva-dl4nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b00560f85506a96f756a20f1ec758ba7e58bab47875298fd1378cbdb6f991790"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
