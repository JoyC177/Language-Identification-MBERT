{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as ex\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "def load_ds(path: str):\n",
    "    with open(path, encoding=\"utf8\") as f:\n",
    "        for l in f:\n",
    "            yield l.rstrip(\"\\n\")\n",
    "\n",
    "x_train = load_ds(\"data/wili-2018/x_train.txt\")\n",
    "y_train = load_ds(\"data/wili-2018/y_train.txt\")\n",
    "x_test = load_ds(\"data/wili-2018/x_test.txt\")\n",
    "y_test = load_ds(\"data/wili-2018/y_test.txt\")\n",
    "\n",
    "x_train = pd.DataFrame(x_train, columns=[\"sentence\"])\n",
    "y_train = pd.DataFrame(y_train, columns=[\"lang\"])\n",
    "x_test = pd.DataFrame(x_test, columns=[\"sentence\"])\n",
    "y_test = pd.DataFrame(y_test, columns=[\"lang\"])\n",
    "\n",
    "# Create a train dev split\n",
    "x_train, x_dev, y_train, y_dev = train_test_split(x_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "train = pd.concat([x_train, y_train], axis=1)\n",
    "dev = pd.concat([x_dev, y_dev], axis=1)\n",
    "test = pd.concat([x_test, y_test], axis=1)\n",
    "langs = sorted(y_train.lang.unique())\n",
    "chars = set(c for s in train.sentence for c in s)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dbe09aee27a4fd98d03ec2c0c458d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3917 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-cased\").to(device)\n",
    "\n",
    "def get_bert_embedding(model, tokenizer, sentences, batch_size=4, shrinkage_fact=1):\n",
    "    with torch.no_grad():\n",
    "        # Create the tensor to house the CLS embeddings\n",
    "        embeddings = torch.zeros((len(sentences) // shrinkage_fact, 768)).to(device)\n",
    "\n",
    "        # Loop over the sentences in batches\n",
    "        for i in tqdm_notebook(range(0, len(sentences) // shrinkage_fact, batch_size)):\n",
    "            # meta = metadata_collector(sentences, device, features)\n",
    "            \n",
    "            encoded_input = tokenizer(list(sentences[i:i+batch_size]), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "            output = model(**encoded_input)\n",
    "\n",
    "            # Select the last hidden state of the token `[CLS]`\n",
    "            last_hidden_states = output[0][:, 0, :]\n",
    "\n",
    "            # Store the embeddings\n",
    "            if i+batch_size < len(embeddings):\n",
    "                embeddings[i:i+batch_size] = last_hidden_states\n",
    "            else:\n",
    "                # Fill up the last ones\n",
    "                embeddings[i:len(embeddings)] = last_hidden_states[:len(embeddings) - i]\n",
    "\n",
    "                last_counter = i\n",
    "                idx = 0\n",
    "                \n",
    "                while last_counter < len(embeddings):\n",
    "                    embeddings[last_counter] = last_hidden_states[idx]\n",
    "                    last_counter += 1\n",
    "                    idx += 1\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "train_embeddings = get_bert_embedding(model, tokenizer, train.sentence, batch_size=24, shrinkage_fact=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b6c70b79ec400eb9d5aa2fbb5679d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dev_embeddings = get_bert_embedding(model, tokenizer, dev.sentence, batch_size=24, shrinkage_fact=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23500, 23500)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev.sentence), len(dev_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94000, 94000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train.sentence), len(train_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAGNOSTIC CLASSIFIER\n",
    "from skorch import NeuralNet\n",
    "from skorch.helper import predefined_split\n",
    "from skorch.dataset import Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "class LinearDiagnosticClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Save dims\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Construct layer\n",
    "        self.layer = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_to_index = {lang: i for i, lang in enumerate(langs)}\n",
    "index_to_language = {i: lang for i, lang in enumerate(langs)}\n",
    "\n",
    "y_dev_id = [language_to_index[lang] for lang in y_dev.lang]\n",
    "y_train_id = [language_to_index[lang] for lang in y_train.lang]\n",
    "valid_ds = Dataset(dev_embeddings, y_dev_id[:len(dev_embeddings)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.1736\u001b[0m        \u001b[32m3.3023\u001b[0m  0.5826\n",
      "      2        \u001b[36m2.7635\u001b[0m        \u001b[32m2.3757\u001b[0m  0.5837\n",
      "      3        \u001b[36m2.0632\u001b[0m        \u001b[32m1.8770\u001b[0m  0.5883\n",
      "      4        \u001b[36m1.6676\u001b[0m        \u001b[32m1.5785\u001b[0m  0.5986\n",
      "      5        \u001b[36m1.4199\u001b[0m        \u001b[32m1.3825\u001b[0m  0.5856\n",
      "      6        \u001b[36m1.2512\u001b[0m        \u001b[32m1.2440\u001b[0m  0.5859\n",
      "      7        \u001b[36m1.1286\u001b[0m        \u001b[32m1.1407\u001b[0m  0.5871\n",
      "      8        \u001b[36m1.0349\u001b[0m        \u001b[32m1.0603\u001b[0m  0.5794\n",
      "      9        \u001b[36m0.9606\u001b[0m        \u001b[32m0.9957\u001b[0m  0.5791\n",
      "     10        \u001b[36m0.9000\u001b[0m        \u001b[32m0.9425\u001b[0m  0.5787\n",
      "     11        \u001b[36m0.8494\u001b[0m        \u001b[32m0.8978\u001b[0m  0.5793\n",
      "     12        \u001b[36m0.8064\u001b[0m        \u001b[32m0.8596\u001b[0m  0.5788\n",
      "     13        \u001b[36m0.7692\u001b[0m        \u001b[32m0.8265\u001b[0m  0.5778\n",
      "     14        \u001b[36m0.7367\u001b[0m        \u001b[32m0.7975\u001b[0m  0.5775\n",
      "     15        \u001b[36m0.7080\u001b[0m        \u001b[32m0.7719\u001b[0m  0.5781\n",
      "     16        \u001b[36m0.6823\u001b[0m        \u001b[32m0.7490\u001b[0m  0.5809\n",
      "     17        \u001b[36m0.6593\u001b[0m        \u001b[32m0.7285\u001b[0m  0.5819\n",
      "     18        \u001b[36m0.6384\u001b[0m        \u001b[32m0.7099\u001b[0m  0.5797\n",
      "     19        \u001b[36m0.6193\u001b[0m        \u001b[32m0.6930\u001b[0m  0.5839\n",
      "     20        \u001b[36m0.6019\u001b[0m        \u001b[32m0.6775\u001b[0m  0.5993\n",
      "     21        \u001b[36m0.5858\u001b[0m        \u001b[32m0.6634\u001b[0m  0.5799\n",
      "     22        \u001b[36m0.5709\u001b[0m        \u001b[32m0.6503\u001b[0m  0.5800\n",
      "     23        \u001b[36m0.5571\u001b[0m        \u001b[32m0.6382\u001b[0m  0.5782\n",
      "     24        \u001b[36m0.5443\u001b[0m        \u001b[32m0.6269\u001b[0m  0.5793\n",
      "     25        \u001b[36m0.5322\u001b[0m        \u001b[32m0.6164\u001b[0m  0.5797\n",
      "     26        \u001b[36m0.5209\u001b[0m        \u001b[32m0.6066\u001b[0m  0.5786\n",
      "     27        \u001b[36m0.5103\u001b[0m        \u001b[32m0.5974\u001b[0m  0.5781\n",
      "     28        \u001b[36m0.5003\u001b[0m        \u001b[32m0.5888\u001b[0m  0.5784\n",
      "     29        \u001b[36m0.4909\u001b[0m        \u001b[32m0.5807\u001b[0m  0.5790\n",
      "     30        \u001b[36m0.4819\u001b[0m        \u001b[32m0.5731\u001b[0m  0.5781\n",
      "     31        \u001b[36m0.4734\u001b[0m        \u001b[32m0.5658\u001b[0m  0.5777\n",
      "     32        \u001b[36m0.4654\u001b[0m        \u001b[32m0.5590\u001b[0m  0.5778\n",
      "     33        \u001b[36m0.4577\u001b[0m        \u001b[32m0.5525\u001b[0m  0.5810\n",
      "     34        \u001b[36m0.4503\u001b[0m        \u001b[32m0.5463\u001b[0m  0.5795\n",
      "     35        \u001b[36m0.4433\u001b[0m        \u001b[32m0.5405\u001b[0m  0.5787\n",
      "     36        \u001b[36m0.4366\u001b[0m        \u001b[32m0.5349\u001b[0m  0.5785\n",
      "     37        \u001b[36m0.4302\u001b[0m        \u001b[32m0.5296\u001b[0m  0.5789\n",
      "     38        \u001b[36m0.4241\u001b[0m        \u001b[32m0.5245\u001b[0m  0.5798\n",
      "     39        \u001b[36m0.4182\u001b[0m        \u001b[32m0.5196\u001b[0m  0.5789\n",
      "     40        \u001b[36m0.4125\u001b[0m        \u001b[32m0.5150\u001b[0m  0.6043\n",
      "     41        \u001b[36m0.4070\u001b[0m        \u001b[32m0.5105\u001b[0m  0.5811\n",
      "     42        \u001b[36m0.4018\u001b[0m        \u001b[32m0.5063\u001b[0m  0.6037\n",
      "     43        \u001b[36m0.3967\u001b[0m        \u001b[32m0.5022\u001b[0m  0.5804\n",
      "     44        \u001b[36m0.3918\u001b[0m        \u001b[32m0.4982\u001b[0m  0.5921\n",
      "     45        \u001b[36m0.3871\u001b[0m        \u001b[32m0.4944\u001b[0m  0.5837\n",
      "     46        \u001b[36m0.3825\u001b[0m        \u001b[32m0.4908\u001b[0m  0.5912\n",
      "     47        \u001b[36m0.3781\u001b[0m        \u001b[32m0.4873\u001b[0m  0.5896\n",
      "     48        \u001b[36m0.3738\u001b[0m        \u001b[32m0.4839\u001b[0m  0.5875\n",
      "     49        \u001b[36m0.3697\u001b[0m        \u001b[32m0.4807\u001b[0m  0.5867\n",
      "     50        \u001b[36m0.3656\u001b[0m        \u001b[32m0.4775\u001b[0m  0.5858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.net.NeuralNet'>[initialized](\n",
       "  module_=LinearDiagnosticClassifier(\n",
       "    (layer): Linear(in_features=768, out_features=235, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = NeuralNet(\n",
    "    module=LinearDiagnosticClassifier,\n",
    "    module__input_dim = 768,\n",
    "    module__output_dim = len(set(y_train.lang)),\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    train_split=predefined_split(valid_ds),\n",
    "    max_epochs=50,\n",
    "    device=device,\n",
    "    verbose=1,\n",
    "    lr=0.2,\n",
    ")\n",
    "\n",
    "net.fit(train_embeddings, y_train_id[:len(train_embeddings)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ace       0.91      0.97      0.94       100\n",
      "         afr       0.97      0.99      0.98       100\n",
      "         als       0.76      0.81      0.78       100\n",
      "         amh       0.82      0.83      0.83       100\n",
      "         ang       0.95      0.90      0.92       100\n",
      "         ara       0.92      0.99      0.95       100\n",
      "         arg       0.99      0.99      0.99       100\n",
      "         arz       0.99      0.93      0.96       100\n",
      "         asm       0.96      0.97      0.97       100\n",
      "         ast       0.93      0.99      0.96       100\n",
      "         ava       0.84      0.80      0.82       100\n",
      "         aym       0.90      0.82      0.86       100\n",
      "         azb       1.00      1.00      1.00       100\n",
      "         aze       1.00      0.99      0.99       100\n",
      "         bak       0.98      0.98      0.98       100\n",
      "         bar       0.88      0.86      0.87       100\n",
      "         bcl       0.92      0.92      0.92       100\n",
      "   be-tarask       0.70      0.80      0.75       100\n",
      "         bel       0.75      0.66      0.70       100\n",
      "         ben       0.96      0.96      0.96       100\n",
      "         bho       0.95      0.82      0.88       100\n",
      "         bjn       0.85      0.89      0.87       100\n",
      "         bod       1.00      0.98      0.99       100\n",
      "         bos       0.74      0.50      0.60       100\n",
      "         bpy       1.00      0.98      0.99       100\n",
      "         bre       0.99      0.96      0.97       100\n",
      "         bul       0.97      0.93      0.95       100\n",
      "         bxr       0.87      0.79      0.83       100\n",
      "         cat       0.92      0.96      0.94       100\n",
      "         cbk       0.78      0.75      0.77       100\n",
      "         cdo       0.92      0.92      0.92       100\n",
      "         ceb       1.00      1.00      1.00       100\n",
      "         ces       1.00      0.96      0.98       100\n",
      "         che       1.00      0.99      0.99       100\n",
      "         chr       0.87      0.84      0.85       100\n",
      "         chv       0.97      0.97      0.97       100\n",
      "         ckb       0.99      0.97      0.98       100\n",
      "         cor       0.97      0.97      0.97       100\n",
      "         cos       0.91      0.80      0.85       100\n",
      "         crh       0.93      0.91      0.92       100\n",
      "         csb       0.99      0.98      0.98       100\n",
      "         cym       0.99      0.98      0.98       100\n",
      "         dan       0.97      0.94      0.95       100\n",
      "         deu       0.92      0.91      0.91       100\n",
      "         diq       0.91      0.87      0.89       100\n",
      "         div       0.87      0.83      0.85       100\n",
      "         dsb       0.75      0.80      0.78       100\n",
      "         dty       0.70      0.64      0.67       100\n",
      "         egl       0.82      0.84      0.83       100\n",
      "         ell       0.99      1.00      1.00       100\n",
      "         eng       0.57      0.89      0.70       100\n",
      "         epo       0.94      0.96      0.95       100\n",
      "         est       0.93      0.98      0.96       100\n",
      "         eus       0.99      0.99      0.99       100\n",
      "         ext       0.92      0.80      0.86       100\n",
      "         fao       0.93      0.87      0.90       100\n",
      "         fas       0.89      0.99      0.94       100\n",
      "         fin       0.97      0.98      0.98       100\n",
      "         fra       0.76      0.84      0.80       100\n",
      "         frp       0.89      0.71      0.79       100\n",
      "         fry       0.99      0.97      0.98       100\n",
      "         fur       0.92      0.81      0.86       100\n",
      "         gag       0.91      0.93      0.92       100\n",
      "         gla       0.99      0.88      0.93       100\n",
      "         gle       0.89      0.99      0.94       100\n",
      "         glg       0.97      0.99      0.98       100\n",
      "         glk       0.87      0.82      0.85       100\n",
      "         glv       0.97      0.98      0.98       100\n",
      "         grn       0.98      0.97      0.97       100\n",
      "         guj       0.98      0.92      0.95       100\n",
      "         hak       0.97      0.91      0.94       100\n",
      "         hat       1.00      0.98      0.99       100\n",
      "         hau       0.93      0.94      0.94       100\n",
      "         hbs       0.52      0.53      0.52       100\n",
      "         heb       0.97      1.00      0.99       100\n",
      "         hif       0.91      0.86      0.88       100\n",
      "         hin       0.90      0.95      0.92       100\n",
      "         hrv       0.51      0.68      0.58       100\n",
      "         hsb       0.82      0.80      0.81       100\n",
      "         hun       1.00      0.98      0.99       100\n",
      "         hye       0.99      0.98      0.98       100\n",
      "         ibo       0.95      0.81      0.88       100\n",
      "         ido       0.97      0.96      0.96       100\n",
      "         ile       0.95      0.90      0.92       100\n",
      "         ilo       0.87      0.95      0.91       100\n",
      "         ina       0.88      0.91      0.89       100\n",
      "         ind       0.78      0.81      0.79       100\n",
      "         isl       0.87      0.94      0.90       100\n",
      "         ita       0.90      0.95      0.93       100\n",
      "         jam       0.93      0.95      0.94       100\n",
      "         jav       0.98      0.85      0.91       100\n",
      "         jbo       1.00      0.99      0.99       100\n",
      "         jpn       0.99      0.98      0.98       100\n",
      "         kaa       0.97      0.95      0.96       100\n",
      "         kab       0.91      0.92      0.92       100\n",
      "         kan       1.00      0.99      0.99       100\n",
      "         kat       0.94      0.91      0.92       100\n",
      "         kaz       0.99      1.00      1.00       100\n",
      "         kbd       0.99      0.98      0.98       100\n",
      "         khm       0.82      0.75      0.79       100\n",
      "         kin       0.78      0.91      0.84       100\n",
      "         kir       0.94      0.99      0.97       100\n",
      "         koi       0.74      0.67      0.70       100\n",
      "         kok       0.90      0.87      0.88       100\n",
      "         kom       0.71      0.63      0.67       100\n",
      "         kor       1.00      0.97      0.98       100\n",
      "         krc       0.95      0.94      0.94       100\n",
      "         ksh       0.83      0.91      0.87       100\n",
      "         kur       0.92      0.93      0.93       100\n",
      "         lad       0.85      0.88      0.87       100\n",
      "         lao       0.75      0.68      0.71       100\n",
      "         lat       0.95      0.94      0.94       100\n",
      "         lav       0.93      0.99      0.96       100\n",
      "         lez       0.88      0.96      0.92       100\n",
      "         lij       0.75      0.88      0.81       100\n",
      "         lim       0.78      0.84      0.81       100\n",
      "         lin       0.88      0.87      0.87       100\n",
      "         lit       1.00      0.98      0.99       100\n",
      "         lmo       0.97      0.95      0.96       100\n",
      "         lrc       0.90      0.87      0.88       100\n",
      "         ltg       0.97      0.94      0.95       100\n",
      "         ltz       0.98      0.98      0.98       100\n",
      "         lug       0.93      0.85      0.89       100\n",
      "         lzh       0.98      0.99      0.99       100\n",
      "         mai       0.74      0.98      0.84       100\n",
      "         mal       1.00      0.99      0.99       100\n",
      "     map-bms       0.77      0.79      0.78       100\n",
      "         mar       0.98      0.93      0.95       100\n",
      "         mdf       0.78      0.93      0.85       100\n",
      "         mhr       0.75      0.92      0.83       100\n",
      "         min       1.00      0.98      0.99       100\n",
      "         mkd       1.00      1.00      1.00       100\n",
      "         mlg       0.99      0.99      0.99       100\n",
      "         mlt       0.96      0.95      0.95       100\n",
      "         mon       0.91      0.96      0.94       100\n",
      "         mri       1.00      0.98      0.99       100\n",
      "         mrj       0.83      0.89      0.86       100\n",
      "         msa       0.93      0.92      0.92       100\n",
      "         mwl       0.87      0.91      0.89       100\n",
      "         mya       1.00      0.99      0.99       100\n",
      "         myv       0.68      0.59      0.63       100\n",
      "         mzn       0.93      0.86      0.90       100\n",
      "         nan       0.91      0.96      0.93       100\n",
      "         nap       0.82      0.74      0.78       100\n",
      "         nav       0.99      1.00      1.00       100\n",
      "         nci       0.91      0.96      0.94       100\n",
      "         nds       0.94      0.96      0.95       100\n",
      "      nds-nl       0.74      0.50      0.60       100\n",
      "         nep       0.80      0.72      0.76       100\n",
      "         new       0.98      0.96      0.97       100\n",
      "         nld       0.94      0.94      0.94       100\n",
      "         nno       0.96      0.92      0.94       100\n",
      "         nob       0.91      0.94      0.93       100\n",
      "         nrm       0.81      0.94      0.87       100\n",
      "         nso       0.97      0.92      0.94       100\n",
      "         oci       0.85      0.88      0.86       100\n",
      "         olo       0.95      0.97      0.96       100\n",
      "         ori       0.95      0.98      0.97       100\n",
      "         orm       0.91      0.88      0.89       100\n",
      "         oss       0.95      1.00      0.98       100\n",
      "         pag       0.97      0.83      0.89       100\n",
      "         pam       0.87      0.88      0.88       100\n",
      "         pan       1.00      0.99      0.99       100\n",
      "         pap       0.90      0.95      0.93       100\n",
      "         pcd       0.73      0.66      0.69       100\n",
      "         pdc       0.80      0.70      0.74       100\n",
      "         pfl       0.73      0.77      0.75       100\n",
      "         pnb       1.00      1.00      1.00       100\n",
      "         pol       0.97      1.00      0.99       100\n",
      "         por       0.96      0.98      0.97       100\n",
      "         pus       0.94      0.89      0.91       100\n",
      "         que       0.90      0.94      0.92       100\n",
      "    roa-tara       0.83      0.85      0.84       100\n",
      "         roh       0.84      0.92      0.88       100\n",
      "         ron       0.97      0.98      0.98       100\n",
      "         rue       0.93      0.95      0.94       100\n",
      "         rup       0.85      0.94      0.90       100\n",
      "         rus       0.80      0.95      0.87       100\n",
      "         sah       0.99      0.90      0.94       100\n",
      "         san       1.00      0.93      0.96       100\n",
      "         scn       0.83      0.97      0.89       100\n",
      "         sco       0.98      0.97      0.97       100\n",
      "         sgs       0.96      0.99      0.98       100\n",
      "         sin       0.68      0.79      0.73       100\n",
      "         slk       0.98      0.98      0.98       100\n",
      "         slv       0.98      0.98      0.98       100\n",
      "         sme       0.89      0.99      0.94       100\n",
      "         sna       0.86      0.89      0.88       100\n",
      "         snd       0.93      0.96      0.95       100\n",
      "         som       0.82      0.93      0.87       100\n",
      "         spa       0.79      0.75      0.77       100\n",
      "         sqi       0.98      0.99      0.99       100\n",
      "         srd       0.92      0.81      0.86       100\n",
      "         srn       0.91      0.93      0.92       100\n",
      "         srp       0.98      0.91      0.94       100\n",
      "         stq       0.93      0.87      0.90       100\n",
      "         sun       1.00      0.96      0.98       100\n",
      "         swa       0.92      0.92      0.92       100\n",
      "         swe       1.00      1.00      1.00       100\n",
      "         szl       0.91      0.90      0.90       100\n",
      "         tam       1.00      0.99      0.99       100\n",
      "         tat       0.99      0.97      0.98       100\n",
      "         tcy       0.97      1.00      0.99       100\n",
      "         tel       0.98      0.96      0.97       100\n",
      "         tet       0.93      0.86      0.90       100\n",
      "         tgk       1.00      0.99      0.99       100\n",
      "         tgl       0.95      0.96      0.96       100\n",
      "         tha       0.95      0.99      0.97       100\n",
      "         ton       0.97      0.98      0.98       100\n",
      "         tsn       0.89      0.96      0.92       100\n",
      "         tuk       1.00      0.98      0.99       100\n",
      "         tur       0.96      0.94      0.95       100\n",
      "         tyv       0.94      0.80      0.86       100\n",
      "         udm       0.93      0.83      0.88       100\n",
      "         uig       1.00      0.97      0.98       100\n",
      "         ukr       0.96      0.98      0.97       100\n",
      "         urd       1.00      0.97      0.98       100\n",
      "         uzb       0.97      0.99      0.98       100\n",
      "         vec       0.95      0.84      0.89       100\n",
      "         vep       0.98      0.93      0.95       100\n",
      "         vie       0.98      0.96      0.97       100\n",
      "         vls       0.74      0.81      0.78       100\n",
      "         vol       0.99      0.97      0.98       100\n",
      "         vro       0.98      0.93      0.95       100\n",
      "         war       1.00      0.99      0.99       100\n",
      "         wln       0.85      0.94      0.89       100\n",
      "         wol       0.90      0.88      0.89       100\n",
      "         wuu       0.95      0.89      0.92       100\n",
      "         xho       0.86      0.88      0.87       100\n",
      "         xmf       0.95      0.92      0.93       100\n",
      "         yid       1.00      0.94      0.97       100\n",
      "         yor       0.96      0.95      0.95       100\n",
      "         zea       0.71      0.77      0.74       100\n",
      "      zh-yue       0.89      0.85      0.87       100\n",
      "         zho       0.88      0.96      0.92       100\n",
      "\n",
      "    accuracy                           0.91     23500\n",
      "   macro avg       0.91      0.91      0.91     23500\n",
      "weighted avg       0.91      0.91      0.91     23500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_y_pred_id = np.argmax(net.predict(dev_embeddings), axis=1)\n",
    "dev_y_pred = [index_to_language[id] for id in dev_y_pred_id]\n",
    "print(classification_report(y_dev[:len(dev_y_pred)], dev_y_pred, target_names=langs, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e8c4bf9a1a9f7e7cc011ed0f1048bb31683f8a7a3e3a00930cb02ac8df58bef2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
