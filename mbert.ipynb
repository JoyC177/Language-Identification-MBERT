{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MBERT Linear Probe Training for Language Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as ex\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "from meta_collector import metadata_collector\n",
    "\n",
    "def load_ds(path: str):\n",
    "    with open(path, encoding=\"utf8\") as f:\n",
    "        for l in f:\n",
    "            yield l.rstrip(\"\\n\")\n",
    "\n",
    "x_train = load_ds(\"data/wili-2018/x_train.txt\")\n",
    "y_train = load_ds(\"data/wili-2018/y_train.txt\")\n",
    "x_test = load_ds(\"data/wili-2018/x_test.txt\")\n",
    "y_test = load_ds(\"data/wili-2018/y_test.txt\")\n",
    "\n",
    "x_train = pd.DataFrame(x_train, columns=[\"sentence\"])\n",
    "y_train = pd.DataFrame(y_train, columns=[\"lang\"])\n",
    "x_test = pd.DataFrame(x_test, columns=[\"sentence\"])\n",
    "y_test = pd.DataFrame(y_test, columns=[\"lang\"])\n",
    "\n",
    "# Create a train dev split\n",
    "x_train, x_dev, y_train, y_dev = train_test_split(x_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "train = pd.concat([x_train, y_train], axis=1)\n",
    "dev = pd.concat([x_dev, y_dev], axis=1)\n",
    "test = pd.concat([x_test, y_test], axis=1)\n",
    "langs = sorted(y_train.lang.unique())\n",
    "chars = set(c for s in train.sentence for c in s)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change here between feature version or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "778\n"
     ]
    }
   ],
   "source": [
    "# Switch here by uncommenting and commenting\n",
    "features = [\"Ll\", \"Zs\", \"Lu\", \"Po\", \"Pd\", \"Lo\", \"Mn\", \"Ps\", \"Pe\", \"Mc\"]\n",
    "# features = []\n",
    "\n",
    "h_dim = 768 + len(features)\n",
    "print(h_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-cased\").to(device)\n",
    "\n",
    "\n",
    "def get_bert_embedding(model, tokenizer, sentences, features, batch_size=4, shrinkage_fact=1):\n",
    "    with torch.no_grad():\n",
    "        # Create the tensor to house the CLS embeddings\n",
    "        embeddings = torch.zeros((len(sentences) // shrinkage_fact, 768+len(features))).to(device)\n",
    "\n",
    "        # Loop over the sentences in batches\n",
    "        for i in tqdm_notebook(range(0, len(sentences) // shrinkage_fact, batch_size)):\n",
    "            \n",
    "            encoded_input = tokenizer(list(sentences[i:i+batch_size]), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "            output = model(**encoded_input)\n",
    "            \n",
    "            if features:\n",
    "                meta = metadata_collector(sentences[i:i+batch_size], device, features)\n",
    "                last_hidden_states = torch.cat((output[0][:, 0, :], meta), 1)\n",
    "            else:\n",
    "                # Select the last hidden state of the token `[CLS]`\n",
    "                last_hidden_states = output[0][:, 0, :]\n",
    "\n",
    "            # Store the embeddings\n",
    "            if i+batch_size <= len(embeddings):\n",
    "                embeddings[i:i+batch_size] = last_hidden_states\n",
    "            else:\n",
    "                # Fill up the last ones\n",
    "                embeddings[i:len(embeddings)] = last_hidden_states[:len(embeddings) - i]\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "train_embeddings = get_bert_embedding(model, tokenizer, train.sentence, features, batch_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dev_embeddings = get_bert_embedding(model, tokenizer, dev.sentence, features, batch_size=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## SAVING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "saving_embeddings = False  # Be careful! Will overwrite the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if saving_embeddings:\n",
    "    # Save train embeddings to disk as a python pickle\n",
    "    # Create a directory for the embeddings if it does not exist yet\n",
    "    if not os.path.exists(\"embeddings\"):\n",
    "        os.mkdir(\"embeddings\")\n",
    "    \n",
    "    if features:\n",
    "        np.save(\"embeddings/train_embeddings_f.npy\", train_embeddings.cpu())\n",
    "        np.save(\"embeddings/dev_embeddings_f.npy\", dev_embeddings.cpu())\n",
    "    else:\n",
    "        np.save(\"embeddings/train_embeddings.npy\", train_embeddings.cpu())\n",
    "        np.save(\"embeddings/dev_embeddings.npy\", dev_embeddings.cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(dev.sentence), len(dev_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(train.sentence), len(train_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with features is loaded.\n"
     ]
    }
   ],
   "source": [
    "loading_embeddings = True\n",
    "\n",
    "if loading_embeddings:\n",
    "    \n",
    "    if features:\n",
    "        train_embeddings = torch.from_numpy(np.load(\"embeddings/train_embeddings_f.npy\")).to(device)\n",
    "        dev_embeddings = torch.from_numpy(np.load(\"embeddings/dev_embeddings_f.npy\")).to(device)\n",
    "        print(\"Model with features is loaded.\")\n",
    "    else:\n",
    "        train_embeddings = torch.from_numpy(np.load(\"embeddings/train_embeddings.npy\")).to(device)\n",
    "        dev_embeddings = torch.from_numpy(np.load(\"embeddings/dev_embeddings.npy\")).to(device)\n",
    "        print(\"Model without features is loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23500, 23500)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev.sentence), len(dev_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94000, 94000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train.sentence), len(train_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# DIAGNOSTIC CLASSIFIER\n",
    "from skorch import NeuralNet\n",
    "from skorch.helper import predefined_split\n",
    "from skorch.dataset import Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "class LinearDiagnosticClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Save dims\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Construct layer\n",
    "        self.layer = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "language_to_index = {lang: i for i, lang in enumerate(langs)}\n",
    "index_to_language = {i: lang for i, lang in enumerate(langs)}\n",
    "\n",
    "y_dev_id = [language_to_index[lang] for lang in y_dev.lang]\n",
    "y_train_id = [language_to_index[lang] for lang in y_train.lang]\n",
    "valid_ds = Dataset(dev_embeddings, y_dev_id[:len(dev_embeddings)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m2.0511\u001b[0m        \u001b[32m1.1200\u001b[0m  0.6182\n",
      "      2        \u001b[36m0.8343\u001b[0m        \u001b[32m0.7946\u001b[0m  0.6189\n",
      "      3        \u001b[36m0.6256\u001b[0m        \u001b[32m0.6619\u001b[0m  0.6193\n",
      "      4        \u001b[36m0.5213\u001b[0m        \u001b[32m0.5926\u001b[0m  0.6432\n",
      "      5        \u001b[36m0.4548\u001b[0m        \u001b[32m0.5494\u001b[0m  0.6310\n",
      "      6        \u001b[36m0.4076\u001b[0m        \u001b[32m0.5188\u001b[0m  0.6485\n",
      "      7        \u001b[36m0.3718\u001b[0m        \u001b[32m0.4952\u001b[0m  0.6359\n",
      "      8        \u001b[36m0.3435\u001b[0m        \u001b[32m0.4763\u001b[0m  0.6254\n",
      "      9        \u001b[36m0.3201\u001b[0m        \u001b[32m0.4610\u001b[0m  0.6286\n",
      "     10        \u001b[36m0.3005\u001b[0m        \u001b[32m0.4485\u001b[0m  0.6181\n",
      "     11        \u001b[36m0.2836\u001b[0m        \u001b[32m0.4382\u001b[0m  0.6226\n",
      "     12        \u001b[36m0.2688\u001b[0m        \u001b[32m0.4296\u001b[0m  0.6215\n",
      "     13        \u001b[36m0.2557\u001b[0m        \u001b[32m0.4224\u001b[0m  0.6253\n",
      "     14        \u001b[36m0.2440\u001b[0m        \u001b[32m0.4162\u001b[0m  0.6358\n",
      "     15        \u001b[36m0.2335\u001b[0m        \u001b[32m0.4109\u001b[0m  0.6373\n",
      "     16        \u001b[36m0.2239\u001b[0m        \u001b[32m0.4063\u001b[0m  0.6361\n",
      "     17        \u001b[36m0.2151\u001b[0m        \u001b[32m0.4023\u001b[0m  0.6305\n",
      "     18        \u001b[36m0.2070\u001b[0m        \u001b[32m0.3988\u001b[0m  0.6244\n",
      "     19        \u001b[36m0.1995\u001b[0m        \u001b[32m0.3957\u001b[0m  0.6412\n",
      "     20        \u001b[36m0.1925\u001b[0m        \u001b[32m0.3929\u001b[0m  0.6415\n",
      "     21        \u001b[36m0.1861\u001b[0m        \u001b[32m0.3905\u001b[0m  0.6183\n",
      "     22        \u001b[36m0.1800\u001b[0m        \u001b[32m0.3884\u001b[0m  0.6393\n",
      "     23        \u001b[36m0.1744\u001b[0m        \u001b[32m0.3864\u001b[0m  0.6547\n",
      "     24        \u001b[36m0.1691\u001b[0m        \u001b[32m0.3847\u001b[0m  0.6486\n",
      "     25        \u001b[36m0.1641\u001b[0m        \u001b[32m0.3832\u001b[0m  0.6328\n",
      "     26        \u001b[36m0.1594\u001b[0m        \u001b[32m0.3819\u001b[0m  0.6364\n",
      "     27        \u001b[36m0.1549\u001b[0m        \u001b[32m0.3807\u001b[0m  0.6405\n",
      "     28        \u001b[36m0.1507\u001b[0m        \u001b[32m0.3796\u001b[0m  0.6499\n",
      "     29        \u001b[36m0.1467\u001b[0m        \u001b[32m0.3787\u001b[0m  0.6380\n",
      "     30        \u001b[36m0.1429\u001b[0m        \u001b[32m0.3778\u001b[0m  0.6420\n",
      "     31        \u001b[36m0.1394\u001b[0m        \u001b[32m0.3771\u001b[0m  0.6402\n",
      "     32        \u001b[36m0.1359\u001b[0m        \u001b[32m0.3764\u001b[0m  0.6493\n",
      "     33        \u001b[36m0.1327\u001b[0m        \u001b[32m0.3757\u001b[0m  0.6538\n",
      "     34        \u001b[36m0.1296\u001b[0m        \u001b[32m0.3751\u001b[0m  0.6593\n",
      "     35        \u001b[36m0.1266\u001b[0m        \u001b[32m0.3746\u001b[0m  0.6449\n",
      "     36        \u001b[36m0.1238\u001b[0m        \u001b[32m0.3741\u001b[0m  0.6494\n",
      "     37        \u001b[36m0.1211\u001b[0m        \u001b[32m0.3737\u001b[0m  0.6472\n",
      "     38        \u001b[36m0.1185\u001b[0m        \u001b[32m0.3733\u001b[0m  0.6465\n",
      "     39        \u001b[36m0.1161\u001b[0m        \u001b[32m0.3730\u001b[0m  0.6465\n",
      "     40        \u001b[36m0.1137\u001b[0m        \u001b[32m0.3726\u001b[0m  0.6490\n",
      "     41        \u001b[36m0.1114\u001b[0m        \u001b[32m0.3724\u001b[0m  0.6606\n",
      "     42        \u001b[36m0.1092\u001b[0m        \u001b[32m0.3721\u001b[0m  0.6441\n",
      "     43        \u001b[36m0.1071\u001b[0m        \u001b[32m0.3719\u001b[0m  0.6427\n",
      "     44        \u001b[36m0.1051\u001b[0m        \u001b[32m0.3717\u001b[0m  0.6371\n",
      "     45        \u001b[36m0.1031\u001b[0m        \u001b[32m0.3715\u001b[0m  0.6412\n",
      "     46        \u001b[36m0.1013\u001b[0m        \u001b[32m0.3713\u001b[0m  0.6487\n",
      "     47        \u001b[36m0.0995\u001b[0m        \u001b[32m0.3712\u001b[0m  0.6525\n",
      "     48        \u001b[36m0.0977\u001b[0m        \u001b[32m0.3710\u001b[0m  0.6482\n",
      "     49        \u001b[36m0.0960\u001b[0m        \u001b[32m0.3709\u001b[0m  0.6341\n",
      "     50        \u001b[36m0.0944\u001b[0m        \u001b[32m0.3708\u001b[0m  0.6415\n",
      "     51        \u001b[36m0.0928\u001b[0m        \u001b[32m0.3707\u001b[0m  0.6419\n",
      "     52        \u001b[36m0.0913\u001b[0m        \u001b[32m0.3707\u001b[0m  0.6411\n",
      "     53        \u001b[36m0.0898\u001b[0m        \u001b[32m0.3706\u001b[0m  0.6494\n",
      "     54        \u001b[36m0.0884\u001b[0m        \u001b[32m0.3705\u001b[0m  0.6416\n",
      "     55        \u001b[36m0.0870\u001b[0m        \u001b[32m0.3705\u001b[0m  0.6360\n",
      "     56        \u001b[36m0.0856\u001b[0m        \u001b[32m0.3704\u001b[0m  0.6343\n",
      "     57        \u001b[36m0.0843\u001b[0m        \u001b[32m0.3704\u001b[0m  0.6351\n",
      "     58        \u001b[36m0.0831\u001b[0m        \u001b[32m0.3703\u001b[0m  0.6394\n",
      "     59        \u001b[36m0.0819\u001b[0m        \u001b[32m0.3703\u001b[0m  0.6382\n",
      "     60        \u001b[36m0.0807\u001b[0m        \u001b[32m0.3703\u001b[0m  0.6458\n",
      "     61        \u001b[36m0.0795\u001b[0m        \u001b[32m0.3703\u001b[0m  0.6426\n",
      "     62        \u001b[36m0.0784\u001b[0m        \u001b[32m0.3703\u001b[0m  0.6421\n",
      "     63        \u001b[36m0.0773\u001b[0m        \u001b[32m0.3703\u001b[0m  0.6403\n",
      "     64        \u001b[36m0.0762\u001b[0m        0.3703  0.6346\n",
      "     65        \u001b[36m0.0752\u001b[0m        0.3703  0.6358\n",
      "     66        \u001b[36m0.0742\u001b[0m        0.3703  0.6388\n",
      "     67        \u001b[36m0.0732\u001b[0m        0.3703  0.6384\n",
      "     68        \u001b[36m0.0723\u001b[0m        0.3703  0.6364\n",
      "     69        \u001b[36m0.0714\u001b[0m        0.3704  0.6361\n",
      "     70        \u001b[36m0.0705\u001b[0m        0.3704  0.6360\n",
      "     71        \u001b[36m0.0696\u001b[0m        0.3704  0.6450\n",
      "     72        \u001b[36m0.0688\u001b[0m        0.3705  0.6338\n",
      "     73        \u001b[36m0.0679\u001b[0m        0.3706  0.6344\n",
      "     74        \u001b[36m0.0671\u001b[0m        0.3706  0.6410\n",
      "     75        \u001b[36m0.0663\u001b[0m        0.3707  0.6398\n",
      "     76        \u001b[36m0.0656\u001b[0m        0.3708  0.6335\n",
      "     77        \u001b[36m0.0648\u001b[0m        0.3709  0.6356\n",
      "     78        \u001b[36m0.0641\u001b[0m        0.3710  0.6359\n",
      "     79        \u001b[36m0.0634\u001b[0m        0.3711  0.6340\n",
      "     80        \u001b[36m0.0627\u001b[0m        0.3712  0.6377\n",
      "     81        \u001b[36m0.0620\u001b[0m        0.3713  0.6346\n",
      "     82        \u001b[36m0.0613\u001b[0m        0.3714  0.6829\n",
      "     83        \u001b[36m0.0607\u001b[0m        0.3715  0.6405\n",
      "     84        \u001b[36m0.0600\u001b[0m        0.3716  0.6365\n",
      "     85        \u001b[36m0.0594\u001b[0m        0.3718  0.6366\n",
      "     86        \u001b[36m0.0588\u001b[0m        0.3719  0.6329\n",
      "     87        \u001b[36m0.0582\u001b[0m        0.3720  0.6266\n",
      "     88        \u001b[36m0.0576\u001b[0m        0.3722  0.6346\n",
      "     89        \u001b[36m0.0570\u001b[0m        0.3723  0.6217\n",
      "     90        \u001b[36m0.0564\u001b[0m        0.3725  0.6225\n",
      "     91        \u001b[36m0.0559\u001b[0m        0.3726  0.6235\n",
      "     92        \u001b[36m0.0553\u001b[0m        0.3728  0.6221\n",
      "     93        \u001b[36m0.0548\u001b[0m        0.3730  0.6346\n",
      "     94        \u001b[36m0.0543\u001b[0m        0.3731  0.6363\n",
      "     95        \u001b[36m0.0538\u001b[0m        0.3733  0.6352\n",
      "     96        \u001b[36m0.0533\u001b[0m        0.3735  0.6237\n",
      "     97        \u001b[36m0.0528\u001b[0m        0.3737  0.6217\n",
      "     98        \u001b[36m0.0523\u001b[0m        0.3739  0.6247\n",
      "     99        \u001b[36m0.0518\u001b[0m        0.3741  0.6461\n",
      "    100        \u001b[36m0.0514\u001b[0m        0.3743  0.6269\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.net.NeuralNet'>[initialized](\n",
       "  module_=LinearDiagnosticClassifier(\n",
       "    (layer): Linear(in_features=778, out_features=235, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = NeuralNet(\n",
    "    module=LinearDiagnosticClassifier,\n",
    "    module__input_dim = h_dim,\n",
    "    module__output_dim = len(set(y_train.lang)),\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    train_split=predefined_split(valid_ds),\n",
    "    max_epochs=100,\n",
    "    device=device,\n",
    "    verbose=1,\n",
    "    optimizer = torch.optim.SGD,\n",
    "    optimizer__momentum=0.9,\n",
    "    optimizer__lr=0.2,\n",
    "#     optimizer = torch.optim.Adam,\n",
    ")\n",
    "\n",
    "net.fit(train_embeddings, y_train_id[:len(train_embeddings)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Save Linear Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "saving_classifier = False # Be careful! Will overwrite the saved probe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if saving_classifier:\n",
    "    # Save train embeddings to disk as a python pickle\n",
    "    # Create a directory for the embeddings if it does not exist yet\n",
    "    if not os.path.exists(\"networks\"):\n",
    "        os.mkdir(\"networks\")\n",
    "    \n",
    "    if features:\n",
    "        torch.save(net, \"networks/linear_probe_f.pt\")\n",
    "    else:\n",
    "        torch.save(net, \"networks/linear_probe.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Linear Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "loading_classifier = True # Be careful! Will overwrite the trained probe in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if loading_classifier:\n",
    "    if features:\n",
    "        net = torch.load(\"networks/linear_probe_f.pt\")\n",
    "    else:\n",
    "        net = torch.load(\"networks/linear_probe.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ace       0.96      0.97      0.97       100\n",
      "         afr       0.98      0.99      0.99       100\n",
      "         als       0.68      0.86      0.76       100\n",
      "         amh       0.98      0.94      0.96       100\n",
      "         ang       0.92      0.94      0.93       100\n",
      "         ara       0.89      0.97      0.93       100\n",
      "         arg       0.99      0.99      0.99       100\n",
      "         arz       0.97      0.88      0.92       100\n",
      "         asm       0.93      0.98      0.96       100\n",
      "         ast       0.92      0.98      0.95       100\n",
      "         ava       0.87      0.79      0.83       100\n",
      "         aym       0.92      0.89      0.90       100\n",
      "         azb       1.00      1.00      1.00       100\n",
      "         aze       0.99      0.98      0.98       100\n",
      "         bak       0.97      0.98      0.98       100\n",
      "         bar       0.85      0.85      0.85       100\n",
      "         bcl       0.94      0.93      0.93       100\n",
      "   be-tarask       0.69      0.79      0.74       100\n",
      "         bel       0.72      0.63      0.67       100\n",
      "         ben       0.98      0.95      0.96       100\n",
      "         bho       0.95      0.87      0.91       100\n",
      "         bjn       0.85      0.94      0.89       100\n",
      "         bod       1.00      0.99      0.99       100\n",
      "         bos       0.65      0.42      0.51       100\n",
      "         bpy       1.00      1.00      1.00       100\n",
      "         bre       0.99      0.96      0.97       100\n",
      "         bul       0.97      0.95      0.96       100\n",
      "         bxr       0.94      0.84      0.89       100\n",
      "         cat       0.97      0.97      0.97       100\n",
      "         cbk       0.74      0.82      0.78       100\n",
      "         cdo       0.95      0.95      0.95       100\n",
      "         ceb       1.00      1.00      1.00       100\n",
      "         ces       0.97      0.96      0.96       100\n",
      "         che       1.00      0.99      0.99       100\n",
      "         chr       0.98      0.98      0.98       100\n",
      "         chv       0.99      0.96      0.97       100\n",
      "         ckb       1.00      0.98      0.99       100\n",
      "         cor       0.99      0.98      0.98       100\n",
      "         cos       0.92      0.82      0.87       100\n",
      "         crh       0.93      0.94      0.94       100\n",
      "         csb       1.00      0.99      0.99       100\n",
      "         cym       0.98      0.99      0.99       100\n",
      "         dan       0.97      0.94      0.95       100\n",
      "         deu       0.92      0.92      0.92       100\n",
      "         diq       0.94      0.87      0.90       100\n",
      "         div       0.98      0.98      0.98       100\n",
      "         dsb       0.81      0.82      0.82       100\n",
      "         dty       0.62      0.73      0.67       100\n",
      "         egl       0.92      0.82      0.87       100\n",
      "         ell       0.98      1.00      0.99       100\n",
      "         eng       0.66      0.81      0.73       100\n",
      "         epo       0.91      0.96      0.94       100\n",
      "         est       0.92      0.98      0.95       100\n",
      "         eus       1.00      0.98      0.99       100\n",
      "         ext       0.88      0.84      0.86       100\n",
      "         fao       0.95      0.89      0.92       100\n",
      "         fas       0.90      0.95      0.93       100\n",
      "         fin       0.99      0.98      0.98       100\n",
      "         fra       0.81      0.83      0.82       100\n",
      "         frp       0.93      0.76      0.84       100\n",
      "         fry       0.97      0.96      0.96       100\n",
      "         fur       0.93      0.86      0.90       100\n",
      "         gag       0.88      0.92      0.90       100\n",
      "         gla       1.00      0.91      0.95       100\n",
      "         gle       0.93      0.99      0.96       100\n",
      "         glg       0.96      0.98      0.97       100\n",
      "         glk       0.84      0.91      0.87       100\n",
      "         glv       0.96      0.98      0.97       100\n",
      "         grn       1.00      1.00      1.00       100\n",
      "         guj       1.00      0.93      0.96       100\n",
      "         hak       0.98      0.93      0.95       100\n",
      "         hat       1.00      1.00      1.00       100\n",
      "         hau       0.98      0.97      0.97       100\n",
      "         hbs       0.48      0.63      0.55       100\n",
      "         heb       0.98      1.00      0.99       100\n",
      "         hif       0.90      0.90      0.90       100\n",
      "         hin       0.88      0.93      0.90       100\n",
      "         hrv       0.55      0.63      0.59       100\n",
      "         hsb       0.87      0.88      0.88       100\n",
      "         hun       1.00      0.99      0.99       100\n",
      "         hye       0.98      0.98      0.98       100\n",
      "         ibo       0.94      0.88      0.91       100\n",
      "         ido       0.98      0.95      0.96       100\n",
      "         ile       0.93      0.91      0.92       100\n",
      "         ilo       0.95      0.94      0.94       100\n",
      "         ina       0.80      0.90      0.85       100\n",
      "         ind       0.87      0.68      0.76       100\n",
      "         isl       0.90      0.95      0.93       100\n",
      "         ita       0.91      0.95      0.93       100\n",
      "         jam       0.98      0.94      0.96       100\n",
      "         jav       0.95      0.78      0.86       100\n",
      "         jbo       1.00      0.99      0.99       100\n",
      "         jpn       0.99      0.97      0.98       100\n",
      "         kaa       0.97      0.98      0.98       100\n",
      "         kab       0.88      0.92      0.90       100\n",
      "         kan       1.00      0.98      0.99       100\n",
      "         kat       0.93      0.93      0.93       100\n",
      "         kaz       1.00      1.00      1.00       100\n",
      "         kbd       1.00      0.99      0.99       100\n",
      "         khm       0.85      0.93      0.89       100\n",
      "         kin       0.80      0.92      0.86       100\n",
      "         kir       0.96      0.99      0.98       100\n",
      "         koi       0.79      0.69      0.74       100\n",
      "         kok       0.94      0.90      0.92       100\n",
      "         kom       0.69      0.75      0.72       100\n",
      "         kor       1.00      0.97      0.98       100\n",
      "         krc       0.95      0.93      0.94       100\n",
      "         ksh       0.87      0.88      0.88       100\n",
      "         kur       0.89      0.95      0.92       100\n",
      "         lad       0.95      0.87      0.91       100\n",
      "         lao       0.82      0.84      0.83       100\n",
      "         lat       0.97      0.94      0.95       100\n",
      "         lav       0.94      0.99      0.97       100\n",
      "         lez       0.87      0.97      0.92       100\n",
      "         lij       0.74      0.94      0.83       100\n",
      "         lim       0.79      0.84      0.82       100\n",
      "         lin       0.86      0.91      0.88       100\n",
      "         lit       1.00      0.98      0.99       100\n",
      "         lmo       0.97      0.95      0.96       100\n",
      "         lrc       0.87      0.86      0.86       100\n",
      "         ltg       0.98      0.96      0.97       100\n",
      "         ltz       1.00      0.97      0.98       100\n",
      "         lug       0.99      0.87      0.93       100\n",
      "         lzh       0.98      1.00      0.99       100\n",
      "         mai       0.83      0.97      0.89       100\n",
      "         mal       1.00      0.99      0.99       100\n",
      "     map-bms       0.71      0.88      0.79       100\n",
      "         mar       0.96      0.95      0.95       100\n",
      "         mdf       0.88      0.92      0.90       100\n",
      "         mhr       0.88      0.92      0.90       100\n",
      "         min       1.00      0.98      0.99       100\n",
      "         mkd       1.00      0.98      0.99       100\n",
      "         mlg       0.99      0.99      0.99       100\n",
      "         mlt       0.96      0.96      0.96       100\n",
      "         mon       0.95      0.96      0.96       100\n",
      "         mri       1.00      0.98      0.99       100\n",
      "         mrj       0.87      0.92      0.89       100\n",
      "         msa       0.90      0.93      0.92       100\n",
      "         mwl       0.90      0.93      0.92       100\n",
      "         mya       1.00      0.99      0.99       100\n",
      "         myv       0.76      0.79      0.77       100\n",
      "         mzn       0.94      0.85      0.89       100\n",
      "         nan       0.98      0.98      0.98       100\n",
      "         nap       0.84      0.80      0.82       100\n",
      "         nav       0.99      1.00      1.00       100\n",
      "         nci       0.93      0.96      0.95       100\n",
      "         nds       0.97      0.95      0.96       100\n",
      "      nds-nl       0.74      0.50      0.60       100\n",
      "         nep       0.77      0.57      0.66       100\n",
      "         new       0.97      0.97      0.97       100\n",
      "         nld       0.92      0.92      0.92       100\n",
      "         nno       0.96      0.94      0.95       100\n",
      "         nob       0.93      0.91      0.92       100\n",
      "         nrm       0.84      0.93      0.88       100\n",
      "         nso       0.99      0.91      0.95       100\n",
      "         oci       0.81      0.92      0.86       100\n",
      "         olo       0.96      0.95      0.95       100\n",
      "         ori       0.96      0.97      0.97       100\n",
      "         orm       0.90      0.90      0.90       100\n",
      "         oss       0.99      1.00      1.00       100\n",
      "         pag       0.95      0.88      0.91       100\n",
      "         pam       0.95      0.92      0.93       100\n",
      "         pan       1.00      0.99      0.99       100\n",
      "         pap       0.94      0.94      0.94       100\n",
      "         pcd       0.75      0.70      0.73       100\n",
      "         pdc       0.87      0.80      0.83       100\n",
      "         pfl       0.79      0.66      0.72       100\n",
      "         pnb       1.00      1.00      1.00       100\n",
      "         pol       0.97      0.98      0.98       100\n",
      "         por       1.00      0.98      0.99       100\n",
      "         pus       0.93      0.89      0.91       100\n",
      "         que       0.95      0.93      0.94       100\n",
      "    roa-tara       0.89      0.86      0.87       100\n",
      "         roh       0.80      0.95      0.87       100\n",
      "         ron       0.99      0.98      0.98       100\n",
      "         rue       0.90      0.92      0.91       100\n",
      "         rup       0.87      0.97      0.92       100\n",
      "         rus       0.81      0.94      0.87       100\n",
      "         sah       0.94      0.93      0.93       100\n",
      "         san       0.99      0.97      0.98       100\n",
      "         scn       0.89      0.96      0.92       100\n",
      "         sco       0.98      0.97      0.97       100\n",
      "         sgs       0.98      1.00      0.99       100\n",
      "         sin       0.93      0.87      0.90       100\n",
      "         slk       0.98      0.97      0.97       100\n",
      "         slv       0.98      0.98      0.98       100\n",
      "         sme       0.95      0.97      0.96       100\n",
      "         sna       0.88      0.91      0.90       100\n",
      "         snd       0.93      0.96      0.95       100\n",
      "         som       0.90      0.92      0.91       100\n",
      "         spa       0.82      0.75      0.79       100\n",
      "         sqi       1.00      0.98      0.99       100\n",
      "         srd       0.93      0.87      0.90       100\n",
      "         srn       0.96      0.97      0.97       100\n",
      "         srp       0.95      0.92      0.93       100\n",
      "         stq       0.92      0.92      0.92       100\n",
      "         sun       0.97      0.96      0.96       100\n",
      "         swa       0.93      0.92      0.92       100\n",
      "         swe       1.00      1.00      1.00       100\n",
      "         szl       0.94      0.92      0.93       100\n",
      "         tam       1.00      0.99      0.99       100\n",
      "         tat       1.00      1.00      1.00       100\n",
      "         tcy       0.97      1.00      0.99       100\n",
      "         tel       1.00      0.97      0.98       100\n",
      "         tet       0.89      0.91      0.90       100\n",
      "         tgk       1.00      0.98      0.99       100\n",
      "         tgl       0.96      0.94      0.95       100\n",
      "         tha       0.99      0.96      0.97       100\n",
      "         ton       0.98      0.98      0.98       100\n",
      "         tsn       0.91      0.98      0.94       100\n",
      "         tuk       1.00      0.97      0.98       100\n",
      "         tur       0.97      0.94      0.95       100\n",
      "         tyv       0.94      0.83      0.88       100\n",
      "         udm       0.86      0.87      0.87       100\n",
      "         uig       1.00      0.97      0.98       100\n",
      "         ukr       0.95      0.99      0.97       100\n",
      "         urd       1.00      0.97      0.98       100\n",
      "         uzb       0.99      0.99      0.99       100\n",
      "         vec       0.92      0.92      0.92       100\n",
      "         vep       0.95      0.93      0.94       100\n",
      "         vie       0.97      0.97      0.97       100\n",
      "         vls       0.81      0.79      0.80       100\n",
      "         vol       1.00      0.99      0.99       100\n",
      "         vro       0.98      0.93      0.95       100\n",
      "         war       1.00      1.00      1.00       100\n",
      "         wln       0.94      0.94      0.94       100\n",
      "         wol       0.94      0.91      0.92       100\n",
      "         wuu       0.94      0.80      0.86       100\n",
      "         xho       0.90      0.91      0.91       100\n",
      "         xmf       0.97      0.94      0.95       100\n",
      "         yid       0.99      0.95      0.97       100\n",
      "         yor       0.96      0.95      0.95       100\n",
      "         zea       0.67      0.85      0.75       100\n",
      "      zh-yue       0.88      0.85      0.86       100\n",
      "         zho       0.85      0.95      0.90       100\n",
      "\n",
      "    accuracy                           0.92     23500\n",
      "   macro avg       0.92      0.92      0.92     23500\n",
      "weighted avg       0.92      0.92      0.92     23500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_y_pred_id = np.argmax(net.predict(dev_embeddings), axis=1)\n",
    "dev_y_pred = [index_to_language[id] for id in dev_y_pred_id]\n",
    "print(classification_report(y_dev[:len(dev_y_pred)], dev_y_pred, target_names=langs, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
