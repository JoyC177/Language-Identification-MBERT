{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MBERT Linear Probe Training for Language Identification"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as ex\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "def load_ds(path: str):\n",
    "    with open(path, encoding=\"utf8\") as f:\n",
    "        for l in f:\n",
    "            yield l.rstrip(\"\\n\")\n",
    "\n",
    "x_train = load_ds(\"data/wili-2018/x_train.txt\")\n",
    "y_train = load_ds(\"data/wili-2018/y_train.txt\")\n",
    "x_test = load_ds(\"data/wili-2018/x_test.txt\")\n",
    "y_test = load_ds(\"data/wili-2018/y_test.txt\")\n",
    "\n",
    "x_train = pd.DataFrame(x_train, columns=[\"sentence\"])\n",
    "y_train = pd.DataFrame(y_train, columns=[\"lang\"])\n",
    "x_test = pd.DataFrame(x_test, columns=[\"sentence\"])\n",
    "y_test = pd.DataFrame(y_test, columns=[\"lang\"])\n",
    "\n",
    "# Create a train dev split\n",
    "x_train, x_dev, y_train, y_dev = train_test_split(x_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "train = pd.concat([x_train, y_train], axis=1)\n",
    "dev = pd.concat([x_dev, y_dev], axis=1)\n",
    "test = pd.concat([x_test, y_test], axis=1)\n",
    "langs = sorted(y_train.lang.unique())\n",
    "chars = set(c for s in train.sentence for c in s)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3917 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a2eef79d89f4261a6db94391a9ca419"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-cased\").to(device)\n",
    "\n",
    "def get_bert_embedding(model, tokenizer, sentences, batch_size=4, shrinkage_fact=1):\n",
    "    with torch.no_grad():\n",
    "        # Create the tensor to house the CLS embeddings\n",
    "        embeddings = torch.zeros((len(sentences) // shrinkage_fact, 768)).to(device)\n",
    "\n",
    "        # Loop over the sentences in batches\n",
    "        for i in tqdm_notebook(range(0, len(sentences) // shrinkage_fact, batch_size)):\n",
    "            encoded_input = tokenizer(list(sentences[i:i+batch_size]), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "            output = model(**encoded_input)\n",
    "\n",
    "            # Select the last hidden state of the token `[CLS]`\n",
    "            last_hidden_states = output[0][:, 0, :]\n",
    "\n",
    "            # Store the embeddings\n",
    "            if i+batch_size <= len(embeddings):\n",
    "                embeddings[i:i+batch_size] = last_hidden_states\n",
    "            else:\n",
    "                # Fill up the last ones\n",
    "                embeddings[i:len(embeddings)] = last_hidden_states[:len(embeddings) - i]\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "train_embeddings = get_bert_embedding(model, tokenizer, train.sentence, batch_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/980 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a44a94f9f0154ed79ca8d57dac3c3273"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dev_embeddings = get_bert_embedding(model, tokenizer, dev.sentence, batch_size=24)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SAVING"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "saving_embeddings = False  # Be careful! Will overwrite the embeddings."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "if saving_embeddings:\n",
    "    # Save train embeddings to disk as a python pickle\n",
    "    # Create a directory for the embeddings if it does not exist yet\n",
    "    if not os.path.exists(\"embeddings\"):\n",
    "        os.mkdir(\"embeddings\")\n",
    "    np.save(\"embeddings/train_embeddings.npy\", train_embeddings.cpu())\n",
    "    np.save(\"embeddings/dev_embeddings.npy\", dev_embeddings.cpu())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(23500, 23500)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev.sentence), len(dev_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(94000, 94000)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train.sentence), len(train_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LOADING"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loading_embeddings = True\n",
    "\n",
    "if loading_embeddings:\n",
    "    train_embeddings = torch.from_numpy(np.load(\"embeddings/train_embeddings.npy\")).to(device)\n",
    "    dev_embeddings = torch.from_numpy(np.load(\"embeddings/dev_embeddings.npy\")).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(dev.sentence), len(dev_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(train.sentence), len(train_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Classifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# DIAGNOSTIC CLASSIFIER\n",
    "from skorch import NeuralNet\n",
    "from skorch.helper import predefined_split\n",
    "from skorch.dataset import Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "class LinearDiagnosticClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Save dims\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Construct layer\n",
    "        self.layer = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "language_to_index = {lang: i for i, lang in enumerate(langs)}\n",
    "index_to_language = {i: lang for i, lang in enumerate(langs)}\n",
    "\n",
    "y_dev_id = [language_to_index[lang] for lang in y_dev.lang]\n",
    "y_train_id = [language_to_index[lang] for lang in y_train.lang]\n",
    "valid_ds = Dataset(dev_embeddings, y_dev_id[:len(dev_embeddings)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net = NeuralNet(\n",
    "    module=LinearDiagnosticClassifier,\n",
    "    module__input_dim = 768,\n",
    "    module__output_dim = len(set(y_train.lang)),\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    train_split=predefined_split(valid_ds),\n",
    "    max_epochs=500,\n",
    "    device=device,\n",
    "    verbose=1,\n",
    "    lr=0.2,\n",
    ")\n",
    "\n",
    "net.fit(train_embeddings, y_train_id[:len(train_embeddings)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save Linear Probe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "saving_classifier = False # Be careful! Will overwrite the saved probe."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if saving_classifier:\n",
    "    # Save train embeddings to disk as a python pickle\n",
    "    # Create a directory for the embeddings if it does not exist yet\n",
    "    if not os.path.exists(\"networks\"):\n",
    "        os.mkdir(\"networks\")\n",
    "    torch.save(net, \"networks/linear_probe.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Linear Probe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "loading_classifier = True # Be careful! Will overwrite the trained probe in memory."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if loading_classifier:\n",
    "    net = torch.load(\"networks/linear_probe.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dev_y_pred_id = np.argmax(net.predict(dev_embeddings), axis=1)\n",
    "dev_y_pred = [index_to_language[id] for id in dev_y_pred_id]\n",
    "print(classification_report(y_dev[:len(dev_y_pred)], dev_y_pred, target_names=langs, zero_division=0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save Linear Probe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "saving_classifier = False # Be careful! Will overwrite the saved probe."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "if saving_classifier:\n",
    "    # Save train embeddings to disk as a python pickle\n",
    "    # Create a directory for the embeddings if it does not exist yet\n",
    "    if not os.path.exists(\"networks\"):\n",
    "        os.mkdir(\"networks\")\n",
    "    torch.save(net, \"networks/linear_probe.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Linear Probe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "loading_classifier = True # Be careful! Will overwrite the trained probe in memory."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "if loading_classifier:\n",
    "    net = torch.load(\"networks/linear_probe.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ace       0.95      0.98      0.97       100\n",
      "         afr       0.97      0.99      0.98       100\n",
      "         als       0.78      0.79      0.79       100\n",
      "         amh       0.86      0.85      0.85       100\n",
      "         ang       0.93      0.94      0.94       100\n",
      "         ara       0.91      0.96      0.93       100\n",
      "         arg       1.00      0.99      0.99       100\n",
      "         arz       0.96      0.90      0.93       100\n",
      "         asm       0.93      0.98      0.96       100\n",
      "         ast       0.94      0.99      0.97       100\n",
      "         ava       0.86      0.83      0.84       100\n",
      "         aym       0.92      0.90      0.91       100\n",
      "         azb       1.00      1.00      1.00       100\n",
      "         aze       0.99      0.98      0.98       100\n",
      "         bak       0.97      0.98      0.98       100\n",
      "         bar       0.84      0.87      0.85       100\n",
      "         bcl       0.95      0.93      0.94       100\n",
      "   be-tarask       0.76      0.78      0.77       100\n",
      "         bel       0.74      0.73      0.74       100\n",
      "         ben       0.98      0.97      0.97       100\n",
      "         bho       0.92      0.89      0.90       100\n",
      "         bjn       0.94      0.92      0.93       100\n",
      "         bod       1.00      0.99      0.99       100\n",
      "         bos       0.67      0.58      0.62       100\n",
      "         bpy       1.00      1.00      1.00       100\n",
      "         bre       0.99      0.95      0.97       100\n",
      "         bul       0.97      0.95      0.96       100\n",
      "         bxr       0.93      0.85      0.89       100\n",
      "         cat       0.98      0.97      0.97       100\n",
      "         cbk       0.77      0.80      0.78       100\n",
      "         cdo       0.98      0.95      0.96       100\n",
      "         ceb       1.00      1.00      1.00       100\n",
      "         ces       0.99      0.96      0.97       100\n",
      "         che       0.99      0.99      0.99       100\n",
      "         chr       0.83      0.86      0.85       100\n",
      "         chv       0.98      0.97      0.97       100\n",
      "         ckb       1.00      0.97      0.98       100\n",
      "         cor       0.99      0.98      0.98       100\n",
      "         cos       0.90      0.90      0.90       100\n",
      "         crh       0.91      0.96      0.94       100\n",
      "         csb       1.00      0.99      0.99       100\n",
      "         cym       0.98      0.98      0.98       100\n",
      "         dan       0.96      0.95      0.95       100\n",
      "         deu       0.91      0.93      0.92       100\n",
      "         diq       0.94      0.87      0.90       100\n",
      "         div       0.92      0.86      0.89       100\n",
      "         dsb       0.82      0.82      0.82       100\n",
      "         dty       0.76      0.70      0.73       100\n",
      "         egl       0.90      0.87      0.88       100\n",
      "         ell       0.98      1.00      0.99       100\n",
      "         eng       0.66      0.85      0.74       100\n",
      "         epo       0.94      0.96      0.95       100\n",
      "         est       0.94      0.98      0.96       100\n",
      "         eus       1.00      0.99      0.99       100\n",
      "         ext       0.87      0.89      0.88       100\n",
      "         fao       0.94      0.92      0.93       100\n",
      "         fas       0.90      0.95      0.93       100\n",
      "         fin       0.98      0.98      0.98       100\n",
      "         fra       0.80      0.83      0.81       100\n",
      "         frp       0.86      0.81      0.84       100\n",
      "         fry       0.98      0.96      0.97       100\n",
      "         fur       0.93      0.93      0.93       100\n",
      "         gag       0.93      0.91      0.92       100\n",
      "         gla       1.00      0.94      0.97       100\n",
      "         gle       0.94      0.99      0.97       100\n",
      "         glg       0.96      0.99      0.98       100\n",
      "         glk       0.86      0.90      0.88       100\n",
      "         glv       0.95      0.98      0.97       100\n",
      "         grn       1.00      1.00      1.00       100\n",
      "         guj       0.99      0.93      0.96       100\n",
      "         hak       0.99      0.94      0.96       100\n",
      "         hat       1.00      1.00      1.00       100\n",
      "         hau       0.96      0.96      0.96       100\n",
      "         hbs       0.55      0.48      0.51       100\n",
      "         heb       0.97      1.00      0.99       100\n",
      "         hif       0.96      0.87      0.91       100\n",
      "         hin       0.89      0.91      0.90       100\n",
      "         hrv       0.51      0.66      0.57       100\n",
      "         hsb       0.89      0.88      0.88       100\n",
      "         hun       1.00      0.99      0.99       100\n",
      "         hye       0.99      0.98      0.98       100\n",
      "         ibo       0.94      0.89      0.91       100\n",
      "         ido       0.98      0.98      0.98       100\n",
      "         ile       0.93      0.95      0.94       100\n",
      "         ilo       0.95      0.95      0.95       100\n",
      "         ina       0.85      0.88      0.87       100\n",
      "         ind       0.84      0.81      0.82       100\n",
      "         isl       0.93      0.94      0.94       100\n",
      "         ita       0.91      0.96      0.94       100\n",
      "         jam       0.98      0.95      0.96       100\n",
      "         jav       0.97      0.89      0.93       100\n",
      "         jbo       1.00      1.00      1.00       100\n",
      "         jpn       0.98      0.98      0.98       100\n",
      "         kaa       0.97      0.97      0.97       100\n",
      "         kab       0.94      0.90      0.92       100\n",
      "         kan       0.99      0.99      0.99       100\n",
      "         kat       0.98      0.94      0.96       100\n",
      "         kaz       1.00      1.00      1.00       100\n",
      "         kbd       1.00      0.99      0.99       100\n",
      "         khm       0.80      0.79      0.79       100\n",
      "         kin       0.81      0.91      0.86       100\n",
      "         kir       0.96      0.99      0.98       100\n",
      "         koi       0.76      0.74      0.75       100\n",
      "         kok       0.93      0.88      0.90       100\n",
      "         kom       0.75      0.71      0.73       100\n",
      "         kor       1.00      0.97      0.98       100\n",
      "         krc       0.95      0.94      0.94       100\n",
      "         ksh       0.88      0.89      0.89       100\n",
      "         kur       0.90      0.95      0.93       100\n",
      "         lad       0.93      0.92      0.92       100\n",
      "         lao       0.75      0.68      0.71       100\n",
      "         lat       0.95      0.94      0.94       100\n",
      "         lav       0.95      0.99      0.97       100\n",
      "         lez       0.92      0.97      0.95       100\n",
      "         lij       0.88      0.92      0.90       100\n",
      "         lim       0.80      0.82      0.81       100\n",
      "         lin       0.90      0.92      0.91       100\n",
      "         lit       1.00      0.98      0.99       100\n",
      "         lmo       0.95      0.94      0.94       100\n",
      "         lrc       0.85      0.89      0.87       100\n",
      "         ltg       0.98      0.96      0.97       100\n",
      "         ltz       1.00      0.98      0.99       100\n",
      "         lug       0.95      0.90      0.92       100\n",
      "         lzh       0.98      1.00      0.99       100\n",
      "         mai       0.84      0.95      0.89       100\n",
      "         mal       0.99      0.99      0.99       100\n",
      "     map-bms       0.80      0.85      0.83       100\n",
      "         mar       0.95      0.95      0.95       100\n",
      "         mdf       0.88      0.91      0.89       100\n",
      "         mhr       0.88      0.92      0.90       100\n",
      "         min       0.99      0.98      0.98       100\n",
      "         mkd       1.00      1.00      1.00       100\n",
      "         mlg       0.99      0.99      0.99       100\n",
      "         mlt       0.97      0.96      0.96       100\n",
      "         mon       0.96      0.97      0.97       100\n",
      "         mri       0.99      0.98      0.98       100\n",
      "         mrj       0.89      0.93      0.91       100\n",
      "         msa       0.93      0.92      0.92       100\n",
      "         mwl       0.94      0.91      0.92       100\n",
      "         mya       1.00      0.99      0.99       100\n",
      "         myv       0.75      0.79      0.77       100\n",
      "         mzn       0.95      0.84      0.89       100\n",
      "         nan       0.98      0.99      0.99       100\n",
      "         nap       0.83      0.78      0.80       100\n",
      "         nav       1.00      1.00      1.00       100\n",
      "         nci       0.96      0.96      0.96       100\n",
      "         nds       0.97      0.95      0.96       100\n",
      "      nds-nl       0.73      0.65      0.69       100\n",
      "         nep       0.78      0.76      0.77       100\n",
      "         new       0.98      0.98      0.98       100\n",
      "         nld       0.92      0.92      0.92       100\n",
      "         nno       0.97      0.92      0.94       100\n",
      "         nob       0.90      0.92      0.91       100\n",
      "         nrm       0.84      0.94      0.89       100\n",
      "         nso       0.98      0.92      0.95       100\n",
      "         oci       0.85      0.90      0.87       100\n",
      "         olo       0.95      0.98      0.97       100\n",
      "         ori       0.96      0.97      0.97       100\n",
      "         orm       0.91      0.90      0.90       100\n",
      "         oss       0.98      1.00      0.99       100\n",
      "         pag       0.93      0.88      0.90       100\n",
      "         pam       0.94      0.92      0.93       100\n",
      "         pan       1.00      0.99      0.99       100\n",
      "         pap       0.93      0.96      0.95       100\n",
      "         pcd       0.74      0.75      0.74       100\n",
      "         pdc       0.91      0.72      0.80       100\n",
      "         pfl       0.67      0.78      0.72       100\n",
      "         pnb       1.00      1.00      1.00       100\n",
      "         pol       0.97      0.98      0.98       100\n",
      "         por       0.99      0.98      0.98       100\n",
      "         pus       0.97      0.89      0.93       100\n",
      "         que       0.97      0.94      0.95       100\n",
      "    roa-tara       0.86      0.85      0.85       100\n",
      "         roh       0.89      0.95      0.92       100\n",
      "         ron       0.97      0.97      0.97       100\n",
      "         rue       0.90      0.93      0.92       100\n",
      "         rup       0.90      0.95      0.93       100\n",
      "         rus       0.80      0.94      0.87       100\n",
      "         sah       0.94      0.91      0.92       100\n",
      "         san       0.98      0.97      0.97       100\n",
      "         scn       0.94      0.95      0.95       100\n",
      "         sco       0.98      0.97      0.97       100\n",
      "         sgs       0.98      1.00      0.99       100\n",
      "         sin       0.69      0.83      0.75       100\n",
      "         slk       0.98      0.98      0.98       100\n",
      "         slv       0.98      0.98      0.98       100\n",
      "         sme       0.95      0.98      0.97       100\n",
      "         sna       0.87      0.91      0.89       100\n",
      "         snd       0.95      0.97      0.96       100\n",
      "         som       0.88      0.92      0.90       100\n",
      "         spa       0.80      0.78      0.79       100\n",
      "         sqi       0.99      0.99      0.99       100\n",
      "         srd       0.91      0.90      0.90       100\n",
      "         srn       0.98      0.96      0.97       100\n",
      "         srp       0.97      0.92      0.94       100\n",
      "         stq       0.92      0.91      0.91       100\n",
      "         sun       0.99      0.96      0.97       100\n",
      "         swa       0.92      0.92      0.92       100\n",
      "         swe       1.00      1.00      1.00       100\n",
      "         szl       0.92      0.93      0.93       100\n",
      "         tam       1.00      0.99      0.99       100\n",
      "         tat       0.99      0.99      0.99       100\n",
      "         tcy       0.98      1.00      0.99       100\n",
      "         tel       1.00      0.96      0.98       100\n",
      "         tet       0.92      0.90      0.91       100\n",
      "         tgk       0.99      0.98      0.98       100\n",
      "         tgl       0.96      0.95      0.95       100\n",
      "         tha       0.98      0.97      0.97       100\n",
      "         ton       0.99      0.98      0.98       100\n",
      "         tsn       0.92      0.98      0.95       100\n",
      "         tuk       1.00      0.98      0.99       100\n",
      "         tur       0.98      0.95      0.96       100\n",
      "         tyv       0.96      0.85      0.90       100\n",
      "         udm       0.90      0.88      0.89       100\n",
      "         uig       1.00      0.98      0.99       100\n",
      "         ukr       0.96      0.99      0.98       100\n",
      "         urd       1.00      0.97      0.98       100\n",
      "         uzb       1.00      0.99      0.99       100\n",
      "         vec       0.91      0.92      0.92       100\n",
      "         vep       0.98      0.95      0.96       100\n",
      "         vie       0.98      0.97      0.97       100\n",
      "         vls       0.77      0.82      0.80       100\n",
      "         vol       1.00      0.99      0.99       100\n",
      "         vro       0.98      0.94      0.96       100\n",
      "         war       0.99      1.00      1.00       100\n",
      "         wln       0.96      0.94      0.95       100\n",
      "         wol       0.91      0.91      0.91       100\n",
      "         wuu       0.93      0.88      0.90       100\n",
      "         xho       0.89      0.91      0.90       100\n",
      "         xmf       0.96      0.96      0.96       100\n",
      "         yid       0.99      0.95      0.97       100\n",
      "         yor       0.96      0.96      0.96       100\n",
      "         zea       0.77      0.76      0.76       100\n",
      "      zh-yue       0.86      0.86      0.86       100\n",
      "         zho       0.90      0.94      0.92       100\n",
      "\n",
      "    accuracy                           0.92     23500\n",
      "   macro avg       0.92      0.92      0.92     23500\n",
      "weighted avg       0.92      0.92      0.92     23500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_y_pred_id = np.argmax(net.predict(dev_embeddings), axis=1)\n",
    "dev_y_pred = [index_to_language[id] for id in dev_y_pred_id]\n",
    "print(classification_report(y_dev[:len(dev_y_pred)], dev_y_pred, target_names=langs, zero_division=0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}