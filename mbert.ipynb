{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as ex\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "def load_ds(path: str):\n",
    "    with open(path, encoding=\"utf8\") as f:\n",
    "        for l in f:\n",
    "            yield l.rstrip(\"\\n\")\n",
    "\n",
    "x_train = load_ds(\"data/wili-2018/x_train.txt\")\n",
    "y_train = load_ds(\"data/wili-2018/y_train.txt\")\n",
    "x_test = load_ds(\"data/wili-2018/x_test.txt\")\n",
    "y_test = load_ds(\"data/wili-2018/y_test.txt\")\n",
    "\n",
    "x_train = pd.DataFrame(x_train, columns=[\"sentence\"])\n",
    "y_train = pd.DataFrame(y_train, columns=[\"lang\"])\n",
    "x_test = pd.DataFrame(x_test, columns=[\"sentence\"])\n",
    "y_test = pd.DataFrame(y_test, columns=[\"lang\"])\n",
    "\n",
    "# Create a train dev split\n",
    "x_train, x_dev, y_train, y_dev = train_test_split(x_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "train = pd.concat([x_train, y_train], axis=1)\n",
    "dev = pd.concat([x_dev, y_dev], axis=1)\n",
    "test = pd.concat([x_test, y_test], axis=1)\n",
    "langs = sorted(y_train.lang.unique())\n",
    "chars = set(c for s in train.sentence for c in s)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27de3a3754a4ed088d06a54966d7cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-cased\").to(device)\n",
    "\n",
    "def get_bert_embedding(model, tokenizer, sentences, batch_size=4, shrinkage_fact=1):\n",
    "    with torch.no_grad():\n",
    "        # Create the tensor to house the CLS embeddings\n",
    "        embeddings = torch.zeros((len(sentences) // shrinkage_fact, 768)).to(device)\n",
    "\n",
    "        # Loop over the sentences in batches\n",
    "        for i in tqdm_notebook(range(0, len(sentences) // shrinkage_fact, batch_size)):\n",
    "            encoded_input = tokenizer(list(sentences[i:i+batch_size]), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "            output = model(**encoded_input)\n",
    "\n",
    "            # Select the last hidden state of the token `[CLS]`\n",
    "            last_hidden_states = output[0][:, 0, :]\n",
    "\n",
    "            # Store the embeddings\n",
    "            if i+batch_size < len(embeddings):\n",
    "                embeddings[i:i+batch_size] = last_hidden_states\n",
    "            else:\n",
    "                # Fill up the last ones\n",
    "                embeddings[i:len(embeddings)] = last_hidden_states[:len(embeddings) - i]\n",
    "\n",
    "                last_counter = i\n",
    "                idx = 0\n",
    "                \n",
    "                while last_counter < len(embeddings):\n",
    "                    embeddings[last_counter] = last_hidden_states[idx]\n",
    "                    last_counter += 1\n",
    "                    idx += 1\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "train_embeddings = get_bert_embedding(model, tokenizer, train.sentence, batch_size=24, shrinkage_fact=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1efcac712e744ddbe76b773885ca912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dev_embeddings = get_bert_embedding(model, tokenizer, dev.sentence, batch_size=24, shrinkage_fact=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23500, 5875)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev.sentence), len(dev_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94000, 23500)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train.sentence), len(train_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAGNOSTIC CLASSIFIER\n",
    "from skorch import NeuralNet\n",
    "from skorch.helper import predefined_split\n",
    "from skorch.dataset import Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "class LinearDiagnosticClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Save dims\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Construct layer\n",
    "        self.layer = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_to_index = {lang: i for i, lang in enumerate(langs)}\n",
    "index_to_language = {i: lang for i, lang in enumerate(langs)}\n",
    "\n",
    "y_dev_id = [language_to_index[lang] for lang in y_dev.lang]\n",
    "y_train_id = [language_to_index[lang] for lang in y_train.lang]\n",
    "valid_ds = Dataset(dev_embeddings, y_dev_id[:len(dev_embeddings)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m5.0040\u001b[0m        \u001b[32m4.6239\u001b[0m  0.1776\n",
      "      2        \u001b[36m4.3192\u001b[0m        \u001b[32m4.0913\u001b[0m  0.1495\n",
      "      3        \u001b[36m3.8154\u001b[0m        \u001b[32m3.6684\u001b[0m  0.1485\n",
      "      4        \u001b[36m3.4069\u001b[0m        \u001b[32m3.3208\u001b[0m  0.1565\n",
      "      5        \u001b[36m3.0695\u001b[0m        \u001b[32m3.0312\u001b[0m  0.1512\n",
      "      6        \u001b[36m2.7881\u001b[0m        \u001b[32m2.7879\u001b[0m  0.1495\n",
      "      7        \u001b[36m2.5517\u001b[0m        \u001b[32m2.5820\u001b[0m  0.1508\n",
      "      8        \u001b[36m2.3517\u001b[0m        \u001b[32m2.4067\u001b[0m  0.1566\n",
      "      9        \u001b[36m2.1811\u001b[0m        \u001b[32m2.2563\u001b[0m  0.1519\n",
      "     10        \u001b[36m2.0346\u001b[0m        \u001b[32m2.1265\u001b[0m  0.1479\n",
      "     11        \u001b[36m1.9078\u001b[0m        \u001b[32m2.0136\u001b[0m  0.1487\n",
      "     12        \u001b[36m1.7974\u001b[0m        \u001b[32m1.9148\u001b[0m  0.1527\n",
      "     13        \u001b[36m1.7004\u001b[0m        \u001b[32m1.8278\u001b[0m  0.1489\n",
      "     14        \u001b[36m1.6148\u001b[0m        \u001b[32m1.7507\u001b[0m  0.1618\n",
      "     15        \u001b[36m1.5386\u001b[0m        \u001b[32m1.6820\u001b[0m  0.1498\n",
      "     16        \u001b[36m1.4705\u001b[0m        \u001b[32m1.6203\u001b[0m  0.1546\n",
      "     17        \u001b[36m1.4092\u001b[0m        \u001b[32m1.5648\u001b[0m  0.1568\n",
      "     18        \u001b[36m1.3537\u001b[0m        \u001b[32m1.5145\u001b[0m  0.1551\n",
      "     19        \u001b[36m1.3033\u001b[0m        \u001b[32m1.4687\u001b[0m  0.1538\n",
      "     20        \u001b[36m1.2573\u001b[0m        \u001b[32m1.4269\u001b[0m  0.1558\n",
      "     21        \u001b[36m1.2152\u001b[0m        \u001b[32m1.3885\u001b[0m  0.1527\n",
      "     22        \u001b[36m1.1763\u001b[0m        \u001b[32m1.3532\u001b[0m  0.1485\n",
      "     23        \u001b[36m1.1404\u001b[0m        \u001b[32m1.3206\u001b[0m  0.1481\n",
      "     24        \u001b[36m1.1072\u001b[0m        \u001b[32m1.2903\u001b[0m  0.1513\n",
      "     25        \u001b[36m1.0762\u001b[0m        \u001b[32m1.2622\u001b[0m  0.1569\n",
      "     26        \u001b[36m1.0473\u001b[0m        \u001b[32m1.2360\u001b[0m  0.1556\n",
      "     27        \u001b[36m1.0203\u001b[0m        \u001b[32m1.2115\u001b[0m  0.1547\n",
      "     28        \u001b[36m0.9950\u001b[0m        \u001b[32m1.1886\u001b[0m  0.1542\n",
      "     29        \u001b[36m0.9712\u001b[0m        \u001b[32m1.1671\u001b[0m  0.1547\n",
      "     30        \u001b[36m0.9487\u001b[0m        \u001b[32m1.1468\u001b[0m  0.1516\n",
      "     31        \u001b[36m0.9275\u001b[0m        \u001b[32m1.1277\u001b[0m  0.1551\n",
      "     32        \u001b[36m0.9075\u001b[0m        \u001b[32m1.1096\u001b[0m  0.1539\n",
      "     33        \u001b[36m0.8885\u001b[0m        \u001b[32m1.0925\u001b[0m  0.1539\n",
      "     34        \u001b[36m0.8704\u001b[0m        \u001b[32m1.0763\u001b[0m  0.1510\n",
      "     35        \u001b[36m0.8532\u001b[0m        \u001b[32m1.0609\u001b[0m  0.1530\n",
      "     36        \u001b[36m0.8369\u001b[0m        \u001b[32m1.0463\u001b[0m  0.1472\n",
      "     37        \u001b[36m0.8212\u001b[0m        \u001b[32m1.0323\u001b[0m  0.1541\n",
      "     38        \u001b[36m0.8063\u001b[0m        \u001b[32m1.0191\u001b[0m  0.1542\n",
      "     39        \u001b[36m0.7920\u001b[0m        \u001b[32m1.0064\u001b[0m  0.1540\n",
      "     40        \u001b[36m0.7784\u001b[0m        \u001b[32m0.9942\u001b[0m  0.1525\n",
      "     41        \u001b[36m0.7652\u001b[0m        \u001b[32m0.9826\u001b[0m  0.1540\n",
      "     42        \u001b[36m0.7526\u001b[0m        \u001b[32m0.9715\u001b[0m  0.1534\n",
      "     43        \u001b[36m0.7405\u001b[0m        \u001b[32m0.9608\u001b[0m  0.1547\n",
      "     44        \u001b[36m0.7289\u001b[0m        \u001b[32m0.9506\u001b[0m  0.1544\n",
      "     45        \u001b[36m0.7177\u001b[0m        \u001b[32m0.9408\u001b[0m  0.1507\n",
      "     46        \u001b[36m0.7069\u001b[0m        \u001b[32m0.9313\u001b[0m  0.1540\n",
      "     47        \u001b[36m0.6964\u001b[0m        \u001b[32m0.9222\u001b[0m  0.1491\n",
      "     48        \u001b[36m0.6863\u001b[0m        \u001b[32m0.9134\u001b[0m  0.1510\n",
      "     49        \u001b[36m0.6766\u001b[0m        \u001b[32m0.9050\u001b[0m  0.1488\n",
      "     50        \u001b[36m0.6672\u001b[0m        \u001b[32m0.8968\u001b[0m  0.1482\n",
      "     51        \u001b[36m0.6581\u001b[0m        \u001b[32m0.8889\u001b[0m  0.1508\n",
      "     52        \u001b[36m0.6492\u001b[0m        \u001b[32m0.8813\u001b[0m  0.1517\n",
      "     53        \u001b[36m0.6407\u001b[0m        \u001b[32m0.8740\u001b[0m  0.1578\n",
      "     54        \u001b[36m0.6324\u001b[0m        \u001b[32m0.8668\u001b[0m  0.1568\n",
      "     55        \u001b[36m0.6243\u001b[0m        \u001b[32m0.8600\u001b[0m  0.1531\n",
      "     56        \u001b[36m0.6165\u001b[0m        \u001b[32m0.8533\u001b[0m  0.1652\n",
      "     57        \u001b[36m0.6089\u001b[0m        \u001b[32m0.8468\u001b[0m  0.1538\n",
      "     58        \u001b[36m0.6015\u001b[0m        \u001b[32m0.8406\u001b[0m  0.1563\n",
      "     59        \u001b[36m0.5943\u001b[0m        \u001b[32m0.8345\u001b[0m  0.1495\n",
      "     60        \u001b[36m0.5874\u001b[0m        \u001b[32m0.8286\u001b[0m  0.1488\n",
      "     61        \u001b[36m0.5806\u001b[0m        \u001b[32m0.8228\u001b[0m  0.1462\n",
      "     62        \u001b[36m0.5739\u001b[0m        \u001b[32m0.8173\u001b[0m  0.1484\n",
      "     63        \u001b[36m0.5675\u001b[0m        \u001b[32m0.8119\u001b[0m  0.1450\n",
      "     64        \u001b[36m0.5612\u001b[0m        \u001b[32m0.8066\u001b[0m  0.1495\n",
      "     65        \u001b[36m0.5550\u001b[0m        \u001b[32m0.8015\u001b[0m  0.1568\n",
      "     66        \u001b[36m0.5491\u001b[0m        \u001b[32m0.7965\u001b[0m  0.1556\n",
      "     67        \u001b[36m0.5432\u001b[0m        \u001b[32m0.7917\u001b[0m  0.1546\n",
      "     68        \u001b[36m0.5375\u001b[0m        \u001b[32m0.7870\u001b[0m  0.1502\n",
      "     69        \u001b[36m0.5319\u001b[0m        \u001b[32m0.7824\u001b[0m  0.1521\n",
      "     70        \u001b[36m0.5265\u001b[0m        \u001b[32m0.7779\u001b[0m  0.1520\n",
      "     71        \u001b[36m0.5212\u001b[0m        \u001b[32m0.7735\u001b[0m  0.1474\n",
      "     72        \u001b[36m0.5160\u001b[0m        \u001b[32m0.7693\u001b[0m  0.1446\n",
      "     73        \u001b[36m0.5109\u001b[0m        \u001b[32m0.7651\u001b[0m  0.1460\n",
      "     74        \u001b[36m0.5059\u001b[0m        \u001b[32m0.7610\u001b[0m  0.1459\n",
      "     75        \u001b[36m0.5010\u001b[0m        \u001b[32m0.7571\u001b[0m  0.1458\n",
      "     76        \u001b[36m0.4962\u001b[0m        \u001b[32m0.7532\u001b[0m  0.1451\n",
      "     77        \u001b[36m0.4916\u001b[0m        \u001b[32m0.7494\u001b[0m  0.1454\n",
      "     78        \u001b[36m0.4870\u001b[0m        \u001b[32m0.7458\u001b[0m  0.1463\n",
      "     79        \u001b[36m0.4825\u001b[0m        \u001b[32m0.7421\u001b[0m  0.1450\n",
      "     80        \u001b[36m0.4781\u001b[0m        \u001b[32m0.7386\u001b[0m  0.1457\n",
      "     81        \u001b[36m0.4738\u001b[0m        \u001b[32m0.7352\u001b[0m  0.1447\n",
      "     82        \u001b[36m0.4695\u001b[0m        \u001b[32m0.7318\u001b[0m  0.1463\n",
      "     83        \u001b[36m0.4654\u001b[0m        \u001b[32m0.7285\u001b[0m  0.1452\n",
      "     84        \u001b[36m0.4613\u001b[0m        \u001b[32m0.7253\u001b[0m  0.1567\n",
      "     85        \u001b[36m0.4573\u001b[0m        \u001b[32m0.7221\u001b[0m  0.1497\n",
      "     86        \u001b[36m0.4534\u001b[0m        \u001b[32m0.7190\u001b[0m  0.1522\n",
      "     87        \u001b[36m0.4496\u001b[0m        \u001b[32m0.7160\u001b[0m  0.1558\n",
      "     88        \u001b[36m0.4458\u001b[0m        \u001b[32m0.7130\u001b[0m  0.1531\n",
      "     89        \u001b[36m0.4421\u001b[0m        \u001b[32m0.7101\u001b[0m  0.1536\n",
      "     90        \u001b[36m0.4384\u001b[0m        \u001b[32m0.7073\u001b[0m  0.1554\n",
      "     91        \u001b[36m0.4349\u001b[0m        \u001b[32m0.7045\u001b[0m  0.1505\n",
      "     92        \u001b[36m0.4313\u001b[0m        \u001b[32m0.7018\u001b[0m  0.1491\n",
      "     93        \u001b[36m0.4279\u001b[0m        \u001b[32m0.6991\u001b[0m  0.1474\n",
      "     94        \u001b[36m0.4245\u001b[0m        \u001b[32m0.6964\u001b[0m  0.1457\n",
      "     95        \u001b[36m0.4211\u001b[0m        \u001b[32m0.6939\u001b[0m  0.1459\n",
      "     96        \u001b[36m0.4178\u001b[0m        \u001b[32m0.6913\u001b[0m  0.1453\n",
      "     97        \u001b[36m0.4146\u001b[0m        \u001b[32m0.6888\u001b[0m  0.1485\n",
      "     98        \u001b[36m0.4114\u001b[0m        \u001b[32m0.6864\u001b[0m  0.1471\n",
      "     99        \u001b[36m0.4083\u001b[0m        \u001b[32m0.6840\u001b[0m  0.1508\n",
      "    100        \u001b[36m0.4052\u001b[0m        \u001b[32m0.6817\u001b[0m  0.1511\n",
      "    101        \u001b[36m0.4022\u001b[0m        \u001b[32m0.6793\u001b[0m  0.1584\n",
      "    102        \u001b[36m0.3992\u001b[0m        \u001b[32m0.6771\u001b[0m  0.1511\n",
      "    103        \u001b[36m0.3963\u001b[0m        \u001b[32m0.6749\u001b[0m  0.1488\n",
      "    104        \u001b[36m0.3934\u001b[0m        \u001b[32m0.6727\u001b[0m  0.1490\n",
      "    105        \u001b[36m0.3905\u001b[0m        \u001b[32m0.6705\u001b[0m  0.1492\n",
      "    106        \u001b[36m0.3877\u001b[0m        \u001b[32m0.6684\u001b[0m  0.1504\n",
      "    107        \u001b[36m0.3849\u001b[0m        \u001b[32m0.6663\u001b[0m  0.1579\n",
      "    108        \u001b[36m0.3822\u001b[0m        \u001b[32m0.6643\u001b[0m  0.1489\n",
      "    109        \u001b[36m0.3795\u001b[0m        \u001b[32m0.6623\u001b[0m  0.1479\n",
      "    110        \u001b[36m0.3769\u001b[0m        \u001b[32m0.6603\u001b[0m  0.1488\n",
      "    111        \u001b[36m0.3743\u001b[0m        \u001b[32m0.6584\u001b[0m  0.1486\n",
      "    112        \u001b[36m0.3717\u001b[0m        \u001b[32m0.6565\u001b[0m  0.1485\n",
      "    113        \u001b[36m0.3692\u001b[0m        \u001b[32m0.6546\u001b[0m  0.1501\n",
      "    114        \u001b[36m0.3667\u001b[0m        \u001b[32m0.6527\u001b[0m  0.1493\n",
      "    115        \u001b[36m0.3642\u001b[0m        \u001b[32m0.6509\u001b[0m  0.1499\n",
      "    116        \u001b[36m0.3618\u001b[0m        \u001b[32m0.6491\u001b[0m  0.1492\n",
      "    117        \u001b[36m0.3594\u001b[0m        \u001b[32m0.6474\u001b[0m  0.1505\n",
      "    118        \u001b[36m0.3570\u001b[0m        \u001b[32m0.6456\u001b[0m  0.1515\n",
      "    119        \u001b[36m0.3547\u001b[0m        \u001b[32m0.6439\u001b[0m  0.1520\n",
      "    120        \u001b[36m0.3524\u001b[0m        \u001b[32m0.6422\u001b[0m  0.1501\n",
      "    121        \u001b[36m0.3501\u001b[0m        \u001b[32m0.6406\u001b[0m  0.1502\n",
      "    122        \u001b[36m0.3478\u001b[0m        \u001b[32m0.6389\u001b[0m  0.1529\n",
      "    123        \u001b[36m0.3456\u001b[0m        \u001b[32m0.6373\u001b[0m  0.1516\n",
      "    124        \u001b[36m0.3434\u001b[0m        \u001b[32m0.6358\u001b[0m  0.1544\n",
      "    125        \u001b[36m0.3413\u001b[0m        \u001b[32m0.6342\u001b[0m  0.1501\n",
      "    126        \u001b[36m0.3391\u001b[0m        \u001b[32m0.6327\u001b[0m  0.1545\n",
      "    127        \u001b[36m0.3370\u001b[0m        \u001b[32m0.6312\u001b[0m  0.1468\n",
      "    128        \u001b[36m0.3350\u001b[0m        \u001b[32m0.6297\u001b[0m  0.1480\n",
      "    129        \u001b[36m0.3329\u001b[0m        \u001b[32m0.6282\u001b[0m  0.1474\n",
      "    130        \u001b[36m0.3309\u001b[0m        \u001b[32m0.6267\u001b[0m  0.1459\n",
      "    131        \u001b[36m0.3289\u001b[0m        \u001b[32m0.6253\u001b[0m  0.1514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    132        \u001b[36m0.3269\u001b[0m        \u001b[32m0.6239\u001b[0m  0.1480\n",
      "    133        \u001b[36m0.3250\u001b[0m        \u001b[32m0.6225\u001b[0m  0.1467\n",
      "    134        \u001b[36m0.3230\u001b[0m        \u001b[32m0.6212\u001b[0m  0.1452\n",
      "    135        \u001b[36m0.3211\u001b[0m        \u001b[32m0.6198\u001b[0m  0.1461\n",
      "    136        \u001b[36m0.3192\u001b[0m        \u001b[32m0.6185\u001b[0m  0.1465\n",
      "    137        \u001b[36m0.3174\u001b[0m        \u001b[32m0.6172\u001b[0m  0.1454\n",
      "    138        \u001b[36m0.3155\u001b[0m        \u001b[32m0.6159\u001b[0m  0.1451\n",
      "    139        \u001b[36m0.3137\u001b[0m        \u001b[32m0.6146\u001b[0m  0.1481\n",
      "    140        \u001b[36m0.3119\u001b[0m        \u001b[32m0.6133\u001b[0m  0.1484\n",
      "    141        \u001b[36m0.3101\u001b[0m        \u001b[32m0.6121\u001b[0m  0.1484\n",
      "    142        \u001b[36m0.3084\u001b[0m        \u001b[32m0.6109\u001b[0m  0.1493\n",
      "    143        \u001b[36m0.3066\u001b[0m        \u001b[32m0.6097\u001b[0m  0.1489\n",
      "    144        \u001b[36m0.3049\u001b[0m        \u001b[32m0.6085\u001b[0m  0.1486\n",
      "    145        \u001b[36m0.3032\u001b[0m        \u001b[32m0.6073\u001b[0m  0.1510\n",
      "    146        \u001b[36m0.3015\u001b[0m        \u001b[32m0.6061\u001b[0m  0.1532\n",
      "    147        \u001b[36m0.2999\u001b[0m        \u001b[32m0.6050\u001b[0m  0.1452\n",
      "    148        \u001b[36m0.2982\u001b[0m        \u001b[32m0.6038\u001b[0m  0.1458\n",
      "    149        \u001b[36m0.2966\u001b[0m        \u001b[32m0.6027\u001b[0m  0.1469\n",
      "    150        \u001b[36m0.2950\u001b[0m        \u001b[32m0.6016\u001b[0m  0.1467\n",
      "    151        \u001b[36m0.2934\u001b[0m        \u001b[32m0.6005\u001b[0m  0.1497\n",
      "    152        \u001b[36m0.2918\u001b[0m        \u001b[32m0.5994\u001b[0m  0.1503\n",
      "    153        \u001b[36m0.2903\u001b[0m        \u001b[32m0.5984\u001b[0m  0.1506\n",
      "    154        \u001b[36m0.2887\u001b[0m        \u001b[32m0.5973\u001b[0m  0.1450\n",
      "    155        \u001b[36m0.2872\u001b[0m        \u001b[32m0.5963\u001b[0m  0.1452\n",
      "    156        \u001b[36m0.2857\u001b[0m        \u001b[32m0.5953\u001b[0m  0.1495\n",
      "    157        \u001b[36m0.2842\u001b[0m        \u001b[32m0.5943\u001b[0m  0.1512\n",
      "    158        \u001b[36m0.2827\u001b[0m        \u001b[32m0.5933\u001b[0m  0.1479\n",
      "    159        \u001b[36m0.2813\u001b[0m        \u001b[32m0.5923\u001b[0m  0.1453\n",
      "    160        \u001b[36m0.2798\u001b[0m        \u001b[32m0.5913\u001b[0m  0.1459\n",
      "    161        \u001b[36m0.2784\u001b[0m        \u001b[32m0.5903\u001b[0m  0.1465\n",
      "    162        \u001b[36m0.2770\u001b[0m        \u001b[32m0.5894\u001b[0m  0.1457\n",
      "    163        \u001b[36m0.2756\u001b[0m        \u001b[32m0.5884\u001b[0m  0.1535\n",
      "    164        \u001b[36m0.2742\u001b[0m        \u001b[32m0.5875\u001b[0m  0.1509\n",
      "    165        \u001b[36m0.2728\u001b[0m        \u001b[32m0.5866\u001b[0m  0.1464\n",
      "    166        \u001b[36m0.2714\u001b[0m        \u001b[32m0.5857\u001b[0m  0.1461\n",
      "    167        \u001b[36m0.2701\u001b[0m        \u001b[32m0.5848\u001b[0m  0.1457\n",
      "    168        \u001b[36m0.2688\u001b[0m        \u001b[32m0.5839\u001b[0m  0.1460\n",
      "    169        \u001b[36m0.2674\u001b[0m        \u001b[32m0.5830\u001b[0m  0.1460\n",
      "    170        \u001b[36m0.2661\u001b[0m        \u001b[32m0.5822\u001b[0m  0.1456\n",
      "    171        \u001b[36m0.2648\u001b[0m        \u001b[32m0.5813\u001b[0m  0.1478\n",
      "    172        \u001b[36m0.2635\u001b[0m        \u001b[32m0.5805\u001b[0m  0.1459\n",
      "    173        \u001b[36m0.2623\u001b[0m        \u001b[32m0.5796\u001b[0m  0.1466\n",
      "    174        \u001b[36m0.2610\u001b[0m        \u001b[32m0.5788\u001b[0m  0.1476\n",
      "    175        \u001b[36m0.2598\u001b[0m        \u001b[32m0.5780\u001b[0m  0.1554\n",
      "    176        \u001b[36m0.2585\u001b[0m        \u001b[32m0.5772\u001b[0m  0.1568\n",
      "    177        \u001b[36m0.2573\u001b[0m        \u001b[32m0.5764\u001b[0m  0.1469\n",
      "    178        \u001b[36m0.2561\u001b[0m        \u001b[32m0.5756\u001b[0m  0.1458\n",
      "    179        \u001b[36m0.2549\u001b[0m        \u001b[32m0.5748\u001b[0m  0.1457\n",
      "    180        \u001b[36m0.2537\u001b[0m        \u001b[32m0.5740\u001b[0m  0.1482\n",
      "    181        \u001b[36m0.2525\u001b[0m        \u001b[32m0.5733\u001b[0m  0.1545\n",
      "    182        \u001b[36m0.2513\u001b[0m        \u001b[32m0.5725\u001b[0m  0.1482\n",
      "    183        \u001b[36m0.2502\u001b[0m        \u001b[32m0.5718\u001b[0m  0.1541\n",
      "    184        \u001b[36m0.2490\u001b[0m        \u001b[32m0.5710\u001b[0m  0.1465\n",
      "    185        \u001b[36m0.2479\u001b[0m        \u001b[32m0.5703\u001b[0m  0.1464\n",
      "    186        \u001b[36m0.2467\u001b[0m        \u001b[32m0.5696\u001b[0m  0.1480\n",
      "    187        \u001b[36m0.2456\u001b[0m        \u001b[32m0.5688\u001b[0m  0.1553\n",
      "    188        \u001b[36m0.2445\u001b[0m        \u001b[32m0.5681\u001b[0m  0.1461\n",
      "    189        \u001b[36m0.2434\u001b[0m        \u001b[32m0.5674\u001b[0m  0.1453\n",
      "    190        \u001b[36m0.2423\u001b[0m        \u001b[32m0.5667\u001b[0m  0.1451\n",
      "    191        \u001b[36m0.2412\u001b[0m        \u001b[32m0.5661\u001b[0m  0.1526\n",
      "    192        \u001b[36m0.2402\u001b[0m        \u001b[32m0.5654\u001b[0m  0.1552\n",
      "    193        \u001b[36m0.2391\u001b[0m        \u001b[32m0.5647\u001b[0m  0.1474\n",
      "    194        \u001b[36m0.2381\u001b[0m        \u001b[32m0.5640\u001b[0m  0.1460\n",
      "    195        \u001b[36m0.2370\u001b[0m        \u001b[32m0.5634\u001b[0m  0.1499\n",
      "    196        \u001b[36m0.2360\u001b[0m        \u001b[32m0.5627\u001b[0m  0.1480\n",
      "    197        \u001b[36m0.2350\u001b[0m        \u001b[32m0.5621\u001b[0m  0.1459\n",
      "    198        \u001b[36m0.2339\u001b[0m        \u001b[32m0.5615\u001b[0m  0.1468\n",
      "    199        \u001b[36m0.2329\u001b[0m        \u001b[32m0.5608\u001b[0m  0.1460\n",
      "    200        \u001b[36m0.2319\u001b[0m        \u001b[32m0.5602\u001b[0m  0.1473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.net.NeuralNet'>[initialized](\n",
       "  module_=LinearDiagnosticClassifier(\n",
       "    (layer): Linear(in_features=768, out_features=235, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = NeuralNet(\n",
    "    module=LinearDiagnosticClassifier,\n",
    "    module__input_dim = 768,\n",
    "    module__output_dim = len(set(y_train.lang)),\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    train_split=predefined_split(valid_ds),\n",
    "    max_epochs=200,\n",
    "    device=device,\n",
    "    verbose=1,\n",
    "    lr=0.2,\n",
    ")\n",
    "\n",
    "net.fit(train_embeddings, y_train_id[:len(train_embeddings)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ace       0.83      0.89      0.86        28\n",
      "         afr       0.95      1.00      0.98        20\n",
      "         als       0.83      0.76      0.79        25\n",
      "         amh       0.90      0.97      0.93        29\n",
      "         ang       0.89      0.83      0.86        30\n",
      "         ara       0.89      0.89      0.89        28\n",
      "         arg       1.00      0.93      0.96        27\n",
      "         arz       0.94      0.88      0.91        33\n",
      "         asm       0.92      0.96      0.94        25\n",
      "         ast       0.96      0.96      0.96        27\n",
      "         ava       0.84      0.78      0.81        27\n",
      "         aym       0.95      0.95      0.95        22\n",
      "         azb       1.00      1.00      1.00        25\n",
      "         aze       1.00      0.92      0.96        25\n",
      "         bak       1.00      1.00      1.00        29\n",
      "         bar       0.75      0.84      0.79        25\n",
      "         bcl       0.84      0.93      0.88        28\n",
      "   be-tarask       0.59      0.76      0.67        25\n",
      "         bel       0.74      0.57      0.64        30\n",
      "         ben       0.87      1.00      0.93        20\n",
      "         bho       0.86      0.96      0.91        25\n",
      "         bjn       0.88      0.81      0.84        26\n",
      "         bod       1.00      1.00      1.00        23\n",
      "         bos       0.38      0.61      0.47        18\n",
      "         bpy       1.00      1.00      1.00        34\n",
      "         bre       1.00      0.97      0.98        29\n",
      "         bul       1.00      0.96      0.98        25\n",
      "         bxr       0.94      0.75      0.83        20\n",
      "         cat       0.91      0.95      0.93        21\n",
      "         cbk       0.84      0.67      0.74        24\n",
      "         cdo       0.97      0.88      0.92        32\n",
      "         ceb       1.00      1.00      1.00        23\n",
      "         ces       0.92      1.00      0.96        22\n",
      "         che       1.00      1.00      1.00        24\n",
      "         chr       0.96      0.83      0.89        29\n",
      "         chv       0.90      1.00      0.95        27\n",
      "         ckb       1.00      0.93      0.96        28\n",
      "         cor       1.00      0.94      0.97        31\n",
      "         cos       0.87      0.95      0.91        21\n",
      "         crh       0.95      0.86      0.90        22\n",
      "         csb       0.97      1.00      0.98        30\n",
      "         cym       1.00      0.96      0.98        24\n",
      "         dan       0.96      0.89      0.93        28\n",
      "         deu       0.96      0.89      0.93        28\n",
      "         diq       1.00      0.79      0.88        24\n",
      "         div       0.70      0.80      0.74        20\n",
      "         dsb       0.78      0.58      0.67        24\n",
      "         dty       0.65      0.60      0.63        25\n",
      "         egl       0.90      0.82      0.86        22\n",
      "         ell       1.00      1.00      1.00        29\n",
      "         eng       0.58      0.96      0.72        26\n",
      "         epo       0.92      0.92      0.92        26\n",
      "         est       0.95      1.00      0.98        21\n",
      "         eus       0.96      1.00      0.98        22\n",
      "         ext       0.89      0.80      0.84        30\n",
      "         fao       0.87      0.93      0.90        28\n",
      "         fas       0.85      1.00      0.92        17\n",
      "         fin       1.00      0.97      0.98        33\n",
      "         fra       0.73      0.88      0.80        25\n",
      "         frp       0.77      0.83      0.80        24\n",
      "         fry       1.00      1.00      1.00        18\n",
      "         fur       0.88      0.81      0.85        27\n",
      "         gag       0.96      0.92      0.94        26\n",
      "         gla       0.86      0.90      0.88        21\n",
      "         gle       0.87      0.95      0.91        21\n",
      "         glg       0.88      1.00      0.93        21\n",
      "         glk       0.80      0.76      0.78        21\n",
      "         glv       0.95      1.00      0.98        20\n",
      "         grn       1.00      0.96      0.98        24\n",
      "         guj       0.93      0.96      0.94        26\n",
      "         hak       0.76      1.00      0.87        26\n",
      "         hat       1.00      1.00      1.00        24\n",
      "         hau       0.91      0.88      0.89        24\n",
      "         hbs       0.50      0.32      0.39        22\n",
      "         heb       1.00      1.00      1.00        27\n",
      "         hif       0.79      0.92      0.85        25\n",
      "         hin       0.97      0.91      0.94        32\n",
      "         hrv       0.43      0.43      0.43        21\n",
      "         hsb       0.81      0.91      0.86        23\n",
      "         hun       1.00      0.96      0.98        27\n",
      "         hye       0.96      1.00      0.98        25\n",
      "         ibo       0.78      0.74      0.76        19\n",
      "         ido       0.94      0.94      0.94        18\n",
      "         ile       0.74      0.85      0.79        20\n",
      "         ilo       0.92      0.96      0.94        24\n",
      "         ina       0.89      0.68      0.77        25\n",
      "         ind       0.78      0.66      0.71        32\n",
      "         isl       0.92      0.85      0.88        27\n",
      "         ita       0.88      0.91      0.89        23\n",
      "         jam       0.93      0.90      0.92        31\n",
      "         jav       0.88      1.00      0.94        22\n",
      "         jbo       1.00      1.00      1.00        19\n",
      "         jpn       1.00      0.88      0.93        24\n",
      "         kaa       0.88      1.00      0.93        21\n",
      "         kab       1.00      0.93      0.96        28\n",
      "         kan       1.00      1.00      1.00        23\n",
      "         kat       1.00      0.95      0.98        21\n",
      "         kaz       1.00      1.00      1.00        23\n",
      "         kbd       1.00      1.00      1.00        26\n",
      "         khm       0.83      0.83      0.83        29\n",
      "         kin       0.74      0.77      0.76        22\n",
      "         kir       0.95      0.95      0.95        21\n",
      "         koi       0.82      0.78      0.80        23\n",
      "         kok       0.90      0.82      0.86        22\n",
      "         kom       0.52      0.81      0.63        16\n",
      "         kor       1.00      0.97      0.98        29\n",
      "         krc       0.96      0.96      0.96        23\n",
      "         ksh       0.66      0.96      0.78        26\n",
      "         kur       0.86      0.89      0.87        27\n",
      "         lad       0.85      0.93      0.89        30\n",
      "         lao       0.62      0.71      0.67        28\n",
      "         lat       0.96      1.00      0.98        25\n",
      "         lav       0.96      1.00      0.98        26\n",
      "         lez       0.90      1.00      0.95        27\n",
      "         lij       0.62      0.92      0.74        25\n",
      "         lim       0.57      0.75      0.65        28\n",
      "         lin       0.96      0.87      0.91        30\n",
      "         lit       1.00      0.89      0.94        28\n",
      "         lmo       0.92      1.00      0.96        22\n",
      "         lrc       0.95      0.95      0.95        22\n",
      "         ltg       1.00      0.97      0.98        30\n",
      "         ltz       0.96      0.96      0.96        23\n",
      "         lug       0.85      0.85      0.85        27\n",
      "         lzh       0.96      1.00      0.98        27\n",
      "         mai       0.86      0.78      0.82        23\n",
      "         mal       1.00      1.00      1.00        19\n",
      "     map-bms       0.78      0.78      0.78        18\n",
      "         mar       0.96      0.92      0.94        26\n",
      "         mdf       0.77      0.87      0.82        23\n",
      "         mhr       0.88      0.75      0.81        28\n",
      "         min       0.95      1.00      0.98        20\n",
      "         mkd       0.93      0.96      0.95        28\n",
      "         mlg       1.00      1.00      1.00        26\n",
      "         mlt       0.92      1.00      0.96        22\n",
      "         mon       0.90      0.90      0.90        21\n",
      "         mri       1.00      0.93      0.97        30\n",
      "         mrj       0.78      0.82      0.80        22\n",
      "         msa       0.81      0.85      0.83        26\n",
      "         mwl       0.96      0.87      0.92        31\n",
      "         mya       1.00      1.00      1.00        19\n",
      "         myv       0.58      0.72      0.64        25\n",
      "         mzn       0.84      0.76      0.80        21\n",
      "         nan       0.95      0.81      0.88        26\n",
      "         nap       0.86      0.63      0.73        19\n",
      "         nav       1.00      1.00      1.00        22\n",
      "         nci       0.85      1.00      0.92        23\n",
      "         nds       1.00      0.92      0.96        25\n",
      "      nds-nl       0.47      0.36      0.41        25\n",
      "         nep       0.62      0.73      0.67        22\n",
      "         new       1.00      0.89      0.94        19\n",
      "         nld       0.87      0.91      0.89        22\n",
      "         nno       0.95      0.87      0.91        23\n",
      "         nob       0.84      0.91      0.87        23\n",
      "         nrm       0.71      0.83      0.77        24\n",
      "         nso       0.92      0.96      0.94        23\n",
      "         oci       0.80      0.77      0.78        26\n",
      "         olo       0.90      1.00      0.95        19\n",
      "         ori       1.00      0.97      0.98        32\n",
      "         orm       0.88      0.92      0.90        25\n",
      "         oss       1.00      1.00      1.00        20\n",
      "         pag       1.00      0.77      0.87        30\n",
      "         pam       0.86      0.90      0.88        21\n",
      "         pan       1.00      1.00      1.00        27\n",
      "         pap       0.88      0.95      0.91        22\n",
      "         pcd       0.84      0.59      0.70        27\n",
      "         pdc       0.67      0.50      0.57        24\n",
      "         pfl       0.75      0.64      0.69        28\n",
      "         pnb       0.89      0.89      0.89        19\n",
      "         pol       1.00      0.96      0.98        23\n",
      "         por       0.96      0.96      0.96        24\n",
      "         pus       1.00      0.76      0.87        17\n",
      "         que       1.00      0.89      0.94        19\n",
      "    roa-tara       0.89      0.61      0.72        28\n",
      "         roh       0.97      0.88      0.92        34\n",
      "         ron       0.90      0.97      0.93        29\n",
      "         rue       0.83      0.91      0.87        22\n",
      "         rup       0.84      0.89      0.86        18\n",
      "         rus       0.82      0.72      0.77        25\n",
      "         sah       0.82      0.96      0.88        24\n",
      "         san       0.95      0.83      0.88        23\n",
      "         scn       1.00      0.96      0.98        26\n",
      "         sco       1.00      0.94      0.97        32\n",
      "         sgs       0.90      1.00      0.95        28\n",
      "         sin       0.75      0.70      0.72        30\n",
      "         slk       0.91      0.91      0.91        23\n",
      "         slv       0.97      1.00      0.99        35\n",
      "         sme       0.74      0.96      0.83        26\n",
      "         sna       0.91      0.88      0.89        33\n",
      "         snd       0.87      1.00      0.93        20\n",
      "         som       0.62      0.88      0.73        17\n",
      "         spa       0.71      0.71      0.71        21\n",
      "         sqi       0.96      0.96      0.96        27\n",
      "         srd       0.73      0.84      0.78        19\n",
      "         srn       0.93      0.86      0.89        29\n",
      "         srp       0.96      0.85      0.90        27\n",
      "         stq       1.00      0.92      0.96        26\n",
      "         sun       0.97      0.97      0.97        29\n",
      "         swa       0.91      0.95      0.93        21\n",
      "         swe       1.00      1.00      1.00        31\n",
      "         szl       0.81      0.81      0.81        27\n",
      "         tam       0.97      0.97      0.97        33\n",
      "         tat       1.00      1.00      1.00        22\n",
      "         tcy       1.00      1.00      1.00        30\n",
      "         tel       0.97      0.92      0.94        37\n",
      "         tet       0.91      0.83      0.87        24\n",
      "         tgk       1.00      1.00      1.00        27\n",
      "         tgl       0.97      1.00      0.98        28\n",
      "         tha       1.00      1.00      1.00        30\n",
      "         ton       0.96      1.00      0.98        26\n",
      "         tsn       0.96      0.92      0.94        25\n",
      "         tuk       0.97      0.97      0.97        35\n",
      "         tur       0.91      0.88      0.89        24\n",
      "         tyv       0.94      0.68      0.79        25\n",
      "         udm       0.82      0.67      0.73        27\n",
      "         uig       1.00      1.00      1.00        27\n",
      "         ukr       0.97      0.97      0.97        29\n",
      "         urd       0.92      0.88      0.90        25\n",
      "         uzb       1.00      0.96      0.98        28\n",
      "         vec       0.91      0.88      0.89        24\n",
      "         vep       1.00      0.95      0.97        20\n",
      "         vie       1.00      0.95      0.98        22\n",
      "         vls       0.71      0.65      0.68        26\n",
      "         vol       1.00      1.00      1.00        19\n",
      "         vro       0.94      1.00      0.97        17\n",
      "         war       1.00      1.00      1.00        25\n",
      "         wln       0.82      0.79      0.81        29\n",
      "         wol       0.78      0.78      0.78        27\n",
      "         wuu       0.97      0.86      0.91        35\n",
      "         xho       0.92      0.92      0.92        25\n",
      "         xmf       0.96      0.96      0.96        23\n",
      "         yid       1.00      0.95      0.98        22\n",
      "         yor       0.96      0.93      0.95        28\n",
      "         zea       0.73      0.67      0.70        24\n",
      "      zh-yue       0.88      0.77      0.82        30\n",
      "         zho       0.67      0.95      0.78        19\n",
      "\n",
      "    accuracy                           0.89      5875\n",
      "   macro avg       0.89      0.89      0.89      5875\n",
      "weighted avg       0.89      0.89      0.89      5875\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_y_pred_id = np.argmax(net.predict(dev_embeddings), axis=1)\n",
    "dev_y_pred = [index_to_language[id] for id in dev_y_pred_id]\n",
    "print(classification_report(y_dev[:len(dev_y_pred)], dev_y_pred, target_names=langs, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
